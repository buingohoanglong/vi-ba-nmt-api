{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"Bản sao của run_train.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"CxLmMnkrj-7f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614851030288,"user_tz":-420,"elapsed":1677267,"user":{"displayName":"Phước Nguyễn Sư","photoUrl":"","userId":"12953302521734949890"}},"outputId":"b745b15c-605b-4f38-d824-aa6b3ab0fbc5"},"source":["# If running with Google Colab, uncomment this\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd \"/content/drive/My Drive/transformer-original\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/My Drive/Documents/Thesis/transformer-original\n","torch.__version__: 1.7.1+cu101\n","opt: Namespace(SGDR=1, batchsize=512, checkpoint=20, create_valset=False, d_model=512, device=0, dropout=0.1, epochs=5, floyd=False, heads=8, load_weights='weights', lr=0.0001, max_strlen=100, n_layers=6, printevery=10, src_data='data/vi-0203-no-augment-shuffle.txt', src_lang='vi', trg_data='data/bana-0203-no-augment-shuffle.txt', trg_lang='en')\n","loading spacy tokenizers...\n","loading presaved fields...\n","creating dataset and iterator... \n","len(SRC.vocab): 6008\n","len(TRG.vocab): 4098\n","loading pretrained weights...\n","model weights will be saved every 20 minutes and at end of epoch to directory weights/\n","training model...\n","training ep 0 - trainlen: 2242\n",">>> time: [09:16:48] training ep: 0 - batch(i): 0 , loss: 1.7445709705352783\n",">>> time: [09:16:49] training ep: 0 - batch(i): 10 , loss: 1.9546805620193481\n",">>> time: [09:16:50] training ep: 0 - batch(i): 20 , loss: 2.282759189605713\n",">>> time: [09:16:51] training ep: 0 - batch(i): 30 , loss: 1.7288033962249756\n",">>> time: [09:16:52] training ep: 0 - batch(i): 40 , loss: 2.7636349201202393\n",">>> time: [09:16:53] training ep: 0 - batch(i): 50 , loss: 1.5528241395950317\n",">>> time: [09:16:54] training ep: 0 - batch(i): 60 , loss: 1.6556482315063477\n",">>> time: [09:16:55] training ep: 0 - batch(i): 70 , loss: 1.7200437784194946\n",">>> time: [09:16:56] training ep: 0 - batch(i): 80 , loss: 1.3699144124984741\n",">>> time: [09:16:57] training ep: 0 - batch(i): 90 , loss: 1.318398118019104\n",">>> time: [09:16:58] training ep: 0 - batch(i): 100 , loss: 2.021174907684326\n",">>> time: [09:16:58] training ep: 0 - batch(i): 110 , loss: 1.3033534288406372\n",">>> time: [09:16:59] training ep: 0 - batch(i): 120 , loss: 2.9079906940460205\n",">>> time: [09:17:00] training ep: 0 - batch(i): 130 , loss: 1.8715736865997314\n",">>> time: [09:17:01] training ep: 0 - batch(i): 140 , loss: 2.228961944580078\n",">>> time: [09:17:02] training ep: 0 - batch(i): 150 , loss: 2.037928819656372\n",">>> time: [09:17:03] training ep: 0 - batch(i): 160 , loss: 2.3926892280578613\n",">>> time: [09:17:04] training ep: 0 - batch(i): 170 , loss: 2.2339046001434326\n",">>> time: [09:17:05] training ep: 0 - batch(i): 180 , loss: 1.2302377223968506\n",">>> time: [09:17:06] training ep: 0 - batch(i): 190 , loss: 1.8352991342544556\n",">>> time: [09:17:07] training ep: 0 - batch(i): 200 , loss: 2.6025233268737793\n",">>> time: [09:17:08] training ep: 0 - batch(i): 210 , loss: 2.056215524673462\n",">>> time: [09:17:09] training ep: 0 - batch(i): 220 , loss: 2.097066879272461\n",">>> time: [09:17:10] training ep: 0 - batch(i): 230 , loss: 1.5479273796081543\n",">>> time: [09:17:10] training ep: 0 - batch(i): 240 , loss: 1.3947769403457642\n",">>> time: [09:17:11] training ep: 0 - batch(i): 250 , loss: 1.8237632513046265\n",">>> time: [09:17:12] training ep: 0 - batch(i): 260 , loss: 1.9310916662216187\n",">>> time: [09:17:13] training ep: 0 - batch(i): 270 , loss: 2.8339881896972656\n",">>> time: [09:17:14] training ep: 0 - batch(i): 280 , loss: 1.2802656888961792\n",">>> time: [09:17:15] training ep: 0 - batch(i): 290 , loss: 1.3429057598114014\n",">>> time: [09:17:16] training ep: 0 - batch(i): 300 , loss: 1.8447952270507812\n",">>> time: [09:17:17] training ep: 0 - batch(i): 310 , loss: 1.741441011428833\n",">>> time: [09:17:17] training ep: 0 - batch(i): 320 , loss: 2.3513026237487793\n",">>> time: [09:17:18] training ep: 0 - batch(i): 330 , loss: 1.3488448858261108\n",">>> time: [09:17:19] training ep: 0 - batch(i): 340 , loss: 1.8402481079101562\n",">>> time: [09:17:20] training ep: 0 - batch(i): 350 , loss: 2.251791477203369\n",">>> time: [09:17:21] training ep: 0 - batch(i): 360 , loss: 2.241093873977661\n",">>> time: [09:17:22] training ep: 0 - batch(i): 370 , loss: 2.1992716789245605\n",">>> time: [09:17:23] training ep: 0 - batch(i): 380 , loss: 2.407288074493408\n",">>> time: [09:17:24] training ep: 0 - batch(i): 390 , loss: 2.595792055130005\n",">>> time: [09:17:25] training ep: 0 - batch(i): 400 , loss: 2.044011354446411\n",">>> time: [09:17:25] training ep: 0 - batch(i): 410 , loss: 1.6407392024993896\n",">>> time: [09:17:26] training ep: 0 - batch(i): 420 , loss: 1.7041513919830322\n",">>> time: [09:17:27] training ep: 0 - batch(i): 430 , loss: 1.8758572340011597\n",">>> time: [09:17:28] training ep: 0 - batch(i): 440 , loss: 1.6791064739227295\n",">>> time: [09:17:29] training ep: 0 - batch(i): 450 , loss: 1.3268002271652222\n",">>> time: [09:17:30] training ep: 0 - batch(i): 460 , loss: 1.9112156629562378\n",">>> time: [09:17:31] training ep: 0 - batch(i): 470 , loss: 2.095724105834961\n",">>> time: [09:17:32] training ep: 0 - batch(i): 480 , loss: 2.2622509002685547\n",">>> time: [09:17:32] training ep: 0 - batch(i): 490 , loss: 2.0682997703552246\n",">>> time: [09:17:33] training ep: 0 - batch(i): 500 , loss: 1.4520702362060547\n",">>> time: [09:17:34] training ep: 0 - batch(i): 510 , loss: 1.3246926069259644\n",">>> time: [09:17:35] training ep: 0 - batch(i): 520 , loss: 1.3590147495269775\n",">>> time: [09:17:36] training ep: 0 - batch(i): 530 , loss: 3.1726977825164795\n",">>> time: [09:17:37] training ep: 0 - batch(i): 540 , loss: 1.3866678476333618\n",">>> time: [09:17:38] training ep: 0 - batch(i): 550 , loss: 1.595585584640503\n",">>> time: [09:17:39] training ep: 0 - batch(i): 560 , loss: 1.6371314525604248\n",">>> time: [09:17:40] training ep: 0 - batch(i): 570 , loss: 1.4556719064712524\n",">>> time: [09:17:40] training ep: 0 - batch(i): 580 , loss: 2.051387071609497\n",">>> time: [09:17:41] training ep: 0 - batch(i): 590 , loss: 2.4841115474700928\n",">>> time: [09:17:42] training ep: 0 - batch(i): 600 , loss: 2.173163652420044\n",">>> time: [09:17:43] training ep: 0 - batch(i): 610 , loss: 1.5495730638504028\n",">>> time: [09:17:44] training ep: 0 - batch(i): 620 , loss: 2.013725996017456\n",">>> time: [09:17:45] training ep: 0 - batch(i): 630 , loss: 1.6409251689910889\n",">>> time: [09:17:46] training ep: 0 - batch(i): 640 , loss: 1.6471679210662842\n",">>> time: [09:17:47] training ep: 0 - batch(i): 650 , loss: 2.3473572731018066\n",">>> time: [09:17:47] training ep: 0 - batch(i): 660 , loss: 2.1312830448150635\n",">>> time: [09:17:48] training ep: 0 - batch(i): 670 , loss: 1.7007524967193604\n",">>> time: [09:17:49] training ep: 0 - batch(i): 680 , loss: 1.736553430557251\n",">>> time: [09:17:50] training ep: 0 - batch(i): 690 , loss: 1.9228729009628296\n",">>> time: [09:17:51] training ep: 0 - batch(i): 700 , loss: 2.793510913848877\n",">>> time: [09:17:52] training ep: 0 - batch(i): 710 , loss: 2.3256607055664062\n",">>> time: [09:17:53] training ep: 0 - batch(i): 720 , loss: 1.2137584686279297\n",">>> time: [09:17:54] training ep: 0 - batch(i): 730 , loss: 1.6802289485931396\n",">>> time: [09:17:54] training ep: 0 - batch(i): 740 , loss: 1.5049190521240234\n",">>> time: [09:17:55] training ep: 0 - batch(i): 750 , loss: 2.69732666015625\n",">>> time: [09:17:56] training ep: 0 - batch(i): 760 , loss: 2.007903814315796\n",">>> time: [09:17:57] training ep: 0 - batch(i): 770 , loss: 1.6441411972045898\n",">>> time: [09:17:58] training ep: 0 - batch(i): 780 , loss: 1.7879427671432495\n",">>> time: [09:17:59] training ep: 0 - batch(i): 790 , loss: 1.2097779512405396\n",">>> time: [09:18:00] training ep: 0 - batch(i): 800 , loss: 1.1941684484481812\n",">>> time: [09:18:01] training ep: 0 - batch(i): 810 , loss: 1.716471552848816\n",">>> time: [09:18:01] training ep: 0 - batch(i): 820 , loss: 1.5112148523330688\n",">>> time: [09:18:02] training ep: 0 - batch(i): 830 , loss: 1.513217806816101\n",">>> time: [09:18:03] training ep: 0 - batch(i): 840 , loss: 2.363743543624878\n",">>> time: [09:18:04] training ep: 0 - batch(i): 850 , loss: 3.1847238540649414\n",">>> time: [09:18:05] training ep: 0 - batch(i): 860 , loss: 2.3796916007995605\n",">>> time: [09:18:06] training ep: 0 - batch(i): 870 , loss: 2.2632012367248535\n",">>> time: [09:18:07] training ep: 0 - batch(i): 880 , loss: 1.6940186023712158\n",">>> time: [09:18:08] training ep: 0 - batch(i): 890 , loss: 2.138619899749756\n",">>> time: [09:18:09] training ep: 0 - batch(i): 900 , loss: 2.3288931846618652\n",">>> time: [09:18:10] training ep: 0 - batch(i): 910 , loss: 1.8963007926940918\n",">>> time: [09:18:10] training ep: 0 - batch(i): 920 , loss: 1.1668680906295776\n",">>> time: [09:18:11] training ep: 0 - batch(i): 930 , loss: 1.5507417917251587\n",">>> time: [09:18:12] training ep: 0 - batch(i): 940 , loss: 1.8304479122161865\n",">>> time: [09:18:13] training ep: 0 - batch(i): 950 , loss: 2.016941785812378\n",">>> time: [09:18:14] training ep: 0 - batch(i): 960 , loss: 1.435969352722168\n",">>> time: [09:18:15] training ep: 0 - batch(i): 970 , loss: 1.9488939046859741\n",">>> time: [09:18:16] training ep: 0 - batch(i): 980 , loss: 1.8803945779800415\n",">>> time: [09:18:17] training ep: 0 - batch(i): 990 , loss: 1.7401541471481323\n",">>> time: [09:18:17] training ep: 0 - batch(i): 1000 , loss: 1.4460874795913696\n",">>> time: [09:18:18] training ep: 0 - batch(i): 1010 , loss: 1.9593759775161743\n",">>> time: [09:18:19] training ep: 0 - batch(i): 1020 , loss: 1.3615262508392334\n",">>> time: [09:18:20] training ep: 0 - batch(i): 1030 , loss: 1.8658007383346558\n",">>> time: [09:18:21] training ep: 0 - batch(i): 1040 , loss: 1.8713476657867432\n",">>> time: [09:18:22] training ep: 0 - batch(i): 1050 , loss: 1.752553105354309\n",">>> time: [09:18:23] training ep: 0 - batch(i): 1060 , loss: 1.2550315856933594\n",">>> time: [09:18:24] training ep: 0 - batch(i): 1070 , loss: 1.8487614393234253\n",">>> time: [09:18:25] training ep: 0 - batch(i): 1080 , loss: 1.3489940166473389\n",">>> time: [09:18:25] training ep: 0 - batch(i): 1090 , loss: 2.317669630050659\n",">>> time: [09:18:26] training ep: 0 - batch(i): 1100 , loss: 2.2370269298553467\n",">>> time: [09:18:27] training ep: 0 - batch(i): 1110 , loss: 0.9392557144165039\n",">>> time: [09:18:28] training ep: 0 - batch(i): 1120 , loss: 1.6384387016296387\n",">>> time: [09:18:29] training ep: 0 - batch(i): 1130 , loss: 2.0778722763061523\n",">>> time: [09:18:30] training ep: 0 - batch(i): 1140 , loss: 1.5909839868545532\n",">>> time: [09:18:31] training ep: 0 - batch(i): 1150 , loss: 1.5330181121826172\n",">>> time: [09:18:32] training ep: 0 - batch(i): 1160 , loss: 1.5085172653198242\n",">>> time: [09:18:33] training ep: 0 - batch(i): 1170 , loss: 1.6797852516174316\n",">>> time: [09:18:33] training ep: 0 - batch(i): 1180 , loss: 1.8199292421340942\n",">>> time: [09:18:34] training ep: 0 - batch(i): 1190 , loss: 1.4735151529312134\n",">>> time: [09:18:35] training ep: 0 - batch(i): 1200 , loss: 1.3798978328704834\n",">>> time: [09:18:36] training ep: 0 - batch(i): 1210 , loss: 1.5901873111724854\n",">>> time: [09:18:37] training ep: 0 - batch(i): 1220 , loss: 2.418972969055176\n",">>> time: [09:18:38] training ep: 0 - batch(i): 1230 , loss: 2.3765790462493896\n",">>> time: [09:18:39] training ep: 0 - batch(i): 1240 , loss: 1.2266249656677246\n",">>> time: [09:18:40] training ep: 0 - batch(i): 1250 , loss: 1.092625379562378\n",">>> time: [09:18:41] training ep: 0 - batch(i): 1260 , loss: 1.731261134147644\n",">>> time: [09:18:41] training ep: 0 - batch(i): 1270 , loss: 2.154409885406494\n",">>> time: [09:18:42] training ep: 0 - batch(i): 1280 , loss: 1.460193395614624\n",">>> time: [09:18:43] training ep: 0 - batch(i): 1290 , loss: 1.0144466161727905\n",">>> time: [09:18:44] training ep: 0 - batch(i): 1300 , loss: 1.4068849086761475\n",">>> time: [09:18:45] training ep: 0 - batch(i): 1310 , loss: 1.4983443021774292\n",">>> time: [09:18:46] training ep: 0 - batch(i): 1320 , loss: 1.2731748819351196\n",">>> time: [09:18:47] training ep: 0 - batch(i): 1330 , loss: 1.6033711433410645\n",">>> time: [09:18:48] training ep: 0 - batch(i): 1340 , loss: 1.162539005279541\n",">>> time: [09:18:48] training ep: 0 - batch(i): 1350 , loss: 1.5448589324951172\n",">>> time: [09:18:49] training ep: 0 - batch(i): 1360 , loss: 1.6557430028915405\n",">>> time: [09:18:50] training ep: 0 - batch(i): 1370 , loss: 1.5369497537612915\n",">>> time: [09:18:51] training ep: 0 - batch(i): 1380 , loss: 1.470692753791809\n",">>> time: [09:18:52] training ep: 0 - batch(i): 1390 , loss: 1.369490623474121\n",">>> time: [09:18:53] training ep: 0 - batch(i): 1400 , loss: 1.436636209487915\n",">>> time: [09:18:54] training ep: 0 - batch(i): 1410 , loss: 1.1780520677566528\n",">>> time: [09:18:55] training ep: 0 - batch(i): 1420 , loss: 1.2145695686340332\n",">>> time: [09:18:56] training ep: 0 - batch(i): 1430 , loss: 2.5758161544799805\n",">>> time: [09:18:56] training ep: 0 - batch(i): 1440 , loss: 1.552785873413086\n",">>> time: [09:18:57] training ep: 0 - batch(i): 1450 , loss: 1.8374135494232178\n",">>> time: [09:18:58] training ep: 0 - batch(i): 1460 , loss: 0.8563275933265686\n",">>> time: [09:18:59] training ep: 0 - batch(i): 1470 , loss: 1.496497392654419\n",">>> time: [09:19:00] training ep: 0 - batch(i): 1480 , loss: 2.4482667446136475\n",">>> time: [09:19:01] training ep: 0 - batch(i): 1490 , loss: 1.1699841022491455\n",">>> time: [09:19:02] training ep: 0 - batch(i): 1500 , loss: 1.6871510744094849\n",">>> time: [09:19:03] training ep: 0 - batch(i): 1510 , loss: 1.3428523540496826\n",">>> time: [09:19:04] training ep: 0 - batch(i): 1520 , loss: 1.7393351793289185\n",">>> time: [09:19:04] training ep: 0 - batch(i): 1530 , loss: 1.2599999904632568\n",">>> time: [09:19:05] training ep: 0 - batch(i): 1540 , loss: 1.6407053470611572\n",">>> time: [09:19:06] training ep: 0 - batch(i): 1550 , loss: 1.5165432691574097\n",">>> time: [09:19:07] training ep: 0 - batch(i): 1560 , loss: 1.5463699102401733\n",">>> time: [09:19:08] training ep: 0 - batch(i): 1570 , loss: 1.326798915863037\n",">>> time: [09:19:09] training ep: 0 - batch(i): 1580 , loss: 2.2216010093688965\n",">>> time: [09:19:10] training ep: 0 - batch(i): 1590 , loss: 1.3584508895874023\n",">>> time: [09:19:11] training ep: 0 - batch(i): 1600 , loss: 1.1710394620895386\n",">>> time: [09:19:12] training ep: 0 - batch(i): 1610 , loss: 1.0635591745376587\n",">>> time: [09:19:13] training ep: 0 - batch(i): 1620 , loss: 1.5606505870819092\n",">>> time: [09:19:13] training ep: 0 - batch(i): 1630 , loss: 1.805643081665039\n",">>> time: [09:19:14] training ep: 0 - batch(i): 1640 , loss: 1.7641758918762207\n",">>> time: [09:19:15] training ep: 0 - batch(i): 1650 , loss: 1.325182318687439\n",">>> time: [09:19:16] training ep: 0 - batch(i): 1660 , loss: 2.3173716068267822\n",">>> time: [09:19:17] training ep: 0 - batch(i): 1670 , loss: 1.656020164489746\n",">>> time: [09:19:18] training ep: 0 - batch(i): 1680 , loss: 1.6368649005889893\n",">>> time: [09:19:19] training ep: 0 - batch(i): 1690 , loss: 1.503852128982544\n",">>> time: [09:19:20] training ep: 0 - batch(i): 1700 , loss: 1.8464561700820923\n",">>> time: [09:19:21] training ep: 0 - batch(i): 1710 , loss: 1.4225865602493286\n",">>> time: [09:19:21] training ep: 0 - batch(i): 1720 , loss: 1.6045911312103271\n",">>> time: [09:19:22] training ep: 0 - batch(i): 1730 , loss: 2.456137180328369\n",">>> time: [09:19:23] training ep: 0 - batch(i): 1740 , loss: 1.164794683456421\n",">>> time: [09:19:24] training ep: 0 - batch(i): 1750 , loss: 1.496041178703308\n",">>> time: [09:19:25] training ep: 0 - batch(i): 1760 , loss: 1.0077714920043945\n",">>> time: [09:19:26] training ep: 0 - batch(i): 1770 , loss: 1.5754249095916748\n",">>> time: [09:19:27] training ep: 0 - batch(i): 1780 , loss: 1.219765543937683\n",">>> time: [09:19:28] training ep: 0 - batch(i): 1790 , loss: 1.0587568283081055\n",">>> time: [09:19:29] training ep: 0 - batch(i): 1800 , loss: 1.4901992082595825\n",">>> time: [09:19:30] training ep: 0 - batch(i): 1810 , loss: 1.1170737743377686\n",">>> time: [09:19:30] training ep: 0 - batch(i): 1820 , loss: 1.050020694732666\n",">>> time: [09:19:31] training ep: 0 - batch(i): 1830 , loss: 0.9186526536941528\n",">>> time: [09:19:32] training ep: 0 - batch(i): 1840 , loss: 1.1863747835159302\n",">>> time: [09:19:33] training ep: 0 - batch(i): 1850 , loss: 1.7355788946151733\n",">>> time: [09:19:34] training ep: 0 - batch(i): 1860 , loss: 1.2776055335998535\n",">>> time: [09:19:35] training ep: 0 - batch(i): 1870 , loss: 1.1926629543304443\n",">>> time: [09:19:36] training ep: 0 - batch(i): 1880 , loss: 1.0964630842208862\n",">>> time: [09:19:37] training ep: 0 - batch(i): 1890 , loss: 1.023327112197876\n",">>> time: [09:19:37] training ep: 0 - batch(i): 1900 , loss: 1.3435982465744019\n",">>> time: [09:19:38] training ep: 0 - batch(i): 1910 , loss: 1.5712944269180298\n",">>> time: [09:19:39] training ep: 0 - batch(i): 1920 , loss: 1.3849714994430542\n",">>> time: [09:19:40] training ep: 0 - batch(i): 1930 , loss: 1.2429497241973877\n",">>> time: [09:19:41] training ep: 0 - batch(i): 1940 , loss: 1.5117024183273315\n",">>> time: [09:19:42] training ep: 0 - batch(i): 1950 , loss: 1.1954622268676758\n",">>> time: [09:19:43] training ep: 0 - batch(i): 1960 , loss: 1.5885839462280273\n",">>> time: [09:19:44] training ep: 0 - batch(i): 1970 , loss: 1.5565677881240845\n",">>> time: [09:19:45] training ep: 0 - batch(i): 1980 , loss: 1.567647933959961\n",">>> time: [09:19:45] training ep: 0 - batch(i): 1990 , loss: 0.9085615277290344\n",">>> time: [09:19:46] training ep: 0 - batch(i): 2000 , loss: 1.2414114475250244\n",">>> time: [09:19:47] training ep: 0 - batch(i): 2010 , loss: 1.507736325263977\n",">>> time: [09:19:48] training ep: 0 - batch(i): 2020 , loss: 1.6566219329833984\n",">>> time: [09:19:49] training ep: 0 - batch(i): 2030 , loss: 1.463120460510254\n",">>> time: [09:19:50] training ep: 0 - batch(i): 2040 , loss: 0.9934673309326172\n",">>> time: [09:19:51] training ep: 0 - batch(i): 2050 , loss: 1.5693284273147583\n",">>> time: [09:19:52] training ep: 0 - batch(i): 2060 , loss: 0.9297923445701599\n",">>> time: [09:19:53] training ep: 0 - batch(i): 2070 , loss: 1.7157963514328003\n",">>> time: [09:19:53] training ep: 0 - batch(i): 2080 , loss: 1.7066740989685059\n",">>> time: [09:19:54] training ep: 0 - batch(i): 2090 , loss: 1.8082376718521118\n",">>> time: [09:19:55] training ep: 0 - batch(i): 2100 , loss: 1.103089690208435\n",">>> time: [09:19:56] training ep: 0 - batch(i): 2110 , loss: 1.3588483333587646\n",">>> time: [09:19:57] training ep: 0 - batch(i): 2120 , loss: 1.0910850763320923\n",">>> time: [09:19:58] training ep: 0 - batch(i): 2130 , loss: 0.9337653517723083\n",">>> time: [09:19:59] training ep: 0 - batch(i): 2140 , loss: 1.0295252799987793\n",">>> time: [09:20:00] training ep: 0 - batch(i): 2150 , loss: 1.3571670055389404\n",">>> time: [09:20:00] training ep: 0 - batch(i): 2160 , loss: 0.8906232714653015\n",">>> time: [09:20:01] training ep: 0 - batch(i): 2170 , loss: 1.484578013420105\n",">>> time: [09:20:02] training ep: 0 - batch(i): 2180 , loss: 1.5403443574905396\n",">>> time: [09:20:03] training ep: 0 - batch(i): 2190 , loss: 1.4359097480773926\n",">>> time: [09:20:04] training ep: 0 - batch(i): 2200 , loss: 2.1635611057281494\n",">>> time: [09:20:05] training ep: 0 - batch(i): 2210 , loss: 0.9093100428581238\n",">>> time: [09:20:06] training ep: 0 - batch(i): 2220 , loss: 1.5110374689102173\n",">>> time: [09:20:07] training ep: 0 - batch(i): 2230 , loss: 1.2264772653579712\n",">>> time: [09:20:07] training ep: 0 - batch(i): 2240 , loss: 1.1400643587112427\n","training ep 1 - trainlen: 2242\n",">>> time: [09:20:09] training ep: 1 - batch(i): 0 , loss: 1.3987942934036255\n",">>> time: [09:20:10] training ep: 1 - batch(i): 10 , loss: 1.0799843072891235\n",">>> time: [09:20:11] training ep: 1 - batch(i): 20 , loss: 1.179603099822998\n",">>> time: [09:20:12] training ep: 1 - batch(i): 30 , loss: 1.3164352178573608\n",">>> time: [09:20:13] training ep: 1 - batch(i): 40 , loss: 1.4291826486587524\n",">>> time: [09:20:14] training ep: 1 - batch(i): 50 , loss: 1.3789665699005127\n",">>> time: [09:20:14] training ep: 1 - batch(i): 60 , loss: 1.1306270360946655\n",">>> time: [09:20:15] training ep: 1 - batch(i): 70 , loss: 1.014643907546997\n",">>> time: [09:20:16] training ep: 1 - batch(i): 80 , loss: 1.3993465900421143\n",">>> time: [09:20:17] training ep: 1 - batch(i): 90 , loss: 1.1704723834991455\n",">>> time: [09:20:18] training ep: 1 - batch(i): 100 , loss: 1.3767198324203491\n",">>> time: [09:20:19] training ep: 1 - batch(i): 110 , loss: 2.208618402481079\n",">>> time: [09:20:20] training ep: 1 - batch(i): 120 , loss: 0.9888961911201477\n",">>> time: [09:20:21] training ep: 1 - batch(i): 130 , loss: 1.437103271484375\n",">>> time: [09:20:22] training ep: 1 - batch(i): 140 , loss: 1.7442477941513062\n",">>> time: [09:20:23] training ep: 1 - batch(i): 150 , loss: 2.456648111343384\n",">>> time: [09:20:24] training ep: 1 - batch(i): 160 , loss: 1.4326564073562622\n",">>> time: [09:20:25] training ep: 1 - batch(i): 170 , loss: 1.7162028551101685\n",">>> time: [09:20:26] training ep: 1 - batch(i): 180 , loss: 1.761067271232605\n",">>> time: [09:20:26] training ep: 1 - batch(i): 190 , loss: 1.2452749013900757\n",">>> time: [09:20:27] training ep: 1 - batch(i): 200 , loss: 1.001620888710022\n",">>> time: [09:20:28] training ep: 1 - batch(i): 210 , loss: 2.179964303970337\n",">>> time: [09:20:29] training ep: 1 - batch(i): 220 , loss: 2.139716625213623\n",">>> time: [09:20:30] training ep: 1 - batch(i): 230 , loss: 1.0966593027114868\n",">>> time: [09:20:31] training ep: 1 - batch(i): 240 , loss: 1.9066307544708252\n",">>> time: [09:20:32] training ep: 1 - batch(i): 250 , loss: 1.1551339626312256\n",">>> time: [09:20:33] training ep: 1 - batch(i): 260 , loss: 1.065952181816101\n",">>> time: [09:20:34] training ep: 1 - batch(i): 270 , loss: 1.480591058731079\n",">>> time: [09:20:35] training ep: 1 - batch(i): 280 , loss: 1.32058846950531\n",">>> time: [09:20:35] training ep: 1 - batch(i): 290 , loss: 1.3488914966583252\n",">>> time: [09:20:36] training ep: 1 - batch(i): 300 , loss: 1.0986628532409668\n",">>> time: [09:20:37] training ep: 1 - batch(i): 310 , loss: 1.9578534364700317\n",">>> time: [09:20:38] training ep: 1 - batch(i): 320 , loss: 2.1652770042419434\n",">>> time: [09:20:39] training ep: 1 - batch(i): 330 , loss: 1.6022672653198242\n",">>> time: [09:20:40] training ep: 1 - batch(i): 340 , loss: 2.285963773727417\n",">>> time: [09:20:41] training ep: 1 - batch(i): 350 , loss: 1.2617436647415161\n",">>> time: [09:20:42] training ep: 1 - batch(i): 360 , loss: 1.5742087364196777\n",">>> time: [09:20:42] training ep: 1 - batch(i): 370 , loss: 1.071445345878601\n",">>> time: [09:20:43] training ep: 1 - batch(i): 380 , loss: 0.9414422512054443\n",">>> time: [09:20:44] training ep: 1 - batch(i): 390 , loss: 1.3944580554962158\n",">>> time: [09:20:45] training ep: 1 - batch(i): 400 , loss: 1.1330904960632324\n",">>> time: [09:20:46] training ep: 1 - batch(i): 410 , loss: 1.5800875425338745\n",">>> time: [09:20:47] training ep: 1 - batch(i): 420 , loss: 1.8664084672927856\n",">>> time: [09:20:48] training ep: 1 - batch(i): 430 , loss: 0.8207163214683533\n",">>> time: [09:20:49] training ep: 1 - batch(i): 440 , loss: 0.6706878542900085\n",">>> time: [09:20:50] training ep: 1 - batch(i): 450 , loss: 0.8604455590248108\n",">>> time: [09:20:51] training ep: 1 - batch(i): 460 , loss: 1.3290939331054688\n",">>> time: [09:20:51] training ep: 1 - batch(i): 470 , loss: 1.5135773420333862\n",">>> time: [09:20:52] training ep: 1 - batch(i): 480 , loss: 1.5216224193572998\n",">>> time: [09:20:53] training ep: 1 - batch(i): 490 , loss: 1.5191994905471802\n",">>> time: [09:20:54] training ep: 1 - batch(i): 500 , loss: 1.6445220708847046\n",">>> time: [09:20:55] training ep: 1 - batch(i): 510 , loss: 0.675896167755127\n",">>> time: [09:20:56] training ep: 1 - batch(i): 520 , loss: 0.7829262018203735\n",">>> time: [09:20:57] training ep: 1 - batch(i): 530 , loss: 1.252017855644226\n",">>> time: [09:20:58] training ep: 1 - batch(i): 540 , loss: 1.6387715339660645\n",">>> time: [09:20:59] training ep: 1 - batch(i): 550 , loss: 0.8685489892959595\n",">>> time: [09:20:59] training ep: 1 - batch(i): 560 , loss: 0.9920095801353455\n",">>> time: [09:21:00] training ep: 1 - batch(i): 570 , loss: 0.8148333430290222\n",">>> time: [09:21:01] training ep: 1 - batch(i): 580 , loss: 1.9519996643066406\n",">>> time: [09:21:02] training ep: 1 - batch(i): 590 , loss: 1.643186330795288\n",">>> time: [09:21:03] training ep: 1 - batch(i): 600 , loss: 1.324332594871521\n",">>> time: [09:21:04] training ep: 1 - batch(i): 610 , loss: 1.4983468055725098\n",">>> time: [09:21:05] training ep: 1 - batch(i): 620 , loss: 1.0051438808441162\n",">>> time: [09:21:06] training ep: 1 - batch(i): 630 , loss: 1.5374021530151367\n",">>> time: [09:21:06] training ep: 1 - batch(i): 640 , loss: 1.9867875576019287\n",">>> time: [09:21:07] training ep: 1 - batch(i): 650 , loss: 1.370413064956665\n",">>> time: [09:21:08] training ep: 1 - batch(i): 660 , loss: 2.0849263668060303\n",">>> time: [09:21:09] training ep: 1 - batch(i): 670 , loss: 1.6190760135650635\n",">>> time: [09:21:10] training ep: 1 - batch(i): 680 , loss: 1.3156243562698364\n",">>> time: [09:21:11] training ep: 1 - batch(i): 690 , loss: 1.6903727054595947\n",">>> time: [09:21:12] training ep: 1 - batch(i): 700 , loss: 1.191116213798523\n",">>> time: [09:21:13] training ep: 1 - batch(i): 710 , loss: 1.357727289199829\n",">>> time: [09:21:14] training ep: 1 - batch(i): 720 , loss: 1.4735817909240723\n",">>> time: [09:21:14] training ep: 1 - batch(i): 730 , loss: 1.728627324104309\n",">>> time: [09:21:15] training ep: 1 - batch(i): 740 , loss: 1.2241629362106323\n",">>> time: [09:21:16] training ep: 1 - batch(i): 750 , loss: 1.004866600036621\n",">>> time: [09:21:17] training ep: 1 - batch(i): 760 , loss: 1.3225470781326294\n",">>> time: [09:21:18] training ep: 1 - batch(i): 770 , loss: 1.1435446739196777\n",">>> time: [09:21:19] training ep: 1 - batch(i): 780 , loss: 1.0503321886062622\n",">>> time: [09:21:20] training ep: 1 - batch(i): 790 , loss: 1.494019865989685\n",">>> time: [09:21:21] training ep: 1 - batch(i): 800 , loss: 1.6033267974853516\n",">>> time: [09:21:22] training ep: 1 - batch(i): 810 , loss: 1.4472754001617432\n",">>> time: [09:21:22] training ep: 1 - batch(i): 820 , loss: 2.406803607940674\n",">>> time: [09:21:23] training ep: 1 - batch(i): 830 , loss: 1.022363305091858\n",">>> time: [09:21:24] training ep: 1 - batch(i): 840 , loss: 1.6738909482955933\n",">>> time: [09:21:25] training ep: 1 - batch(i): 850 , loss: 1.1227840185165405\n",">>> time: [09:21:26] training ep: 1 - batch(i): 860 , loss: 1.2771116495132446\n",">>> time: [09:21:27] training ep: 1 - batch(i): 870 , loss: 1.440084457397461\n",">>> time: [09:21:28] training ep: 1 - batch(i): 880 , loss: 1.2908776998519897\n",">>> time: [09:21:29] training ep: 1 - batch(i): 890 , loss: 2.6474955081939697\n",">>> time: [09:21:30] training ep: 1 - batch(i): 900 , loss: 1.0817351341247559\n",">>> time: [09:21:30] training ep: 1 - batch(i): 910 , loss: 0.8618136048316956\n",">>> time: [09:21:31] training ep: 1 - batch(i): 920 , loss: 1.219423532485962\n",">>> time: [09:21:32] training ep: 1 - batch(i): 930 , loss: 1.0671627521514893\n",">>> time: [09:21:33] training ep: 1 - batch(i): 940 , loss: 2.0566554069519043\n",">>> time: [09:21:34] training ep: 1 - batch(i): 950 , loss: 1.176802158355713\n",">>> time: [09:21:35] training ep: 1 - batch(i): 960 , loss: 1.734444499015808\n",">>> time: [09:21:36] training ep: 1 - batch(i): 970 , loss: 0.8065052628517151\n",">>> time: [09:21:37] training ep: 1 - batch(i): 980 , loss: 1.006615400314331\n",">>> time: [09:21:38] training ep: 1 - batch(i): 990 , loss: 1.3017699718475342\n",">>> time: [09:21:39] training ep: 1 - batch(i): 1000 , loss: 1.5964088439941406\n",">>> time: [09:21:39] training ep: 1 - batch(i): 1010 , loss: 1.6933282613754272\n",">>> time: [09:21:40] training ep: 1 - batch(i): 1020 , loss: 0.8930481672286987\n",">>> time: [09:21:41] training ep: 1 - batch(i): 1030 , loss: 1.0408211946487427\n",">>> time: [09:21:42] training ep: 1 - batch(i): 1040 , loss: 1.1138861179351807\n",">>> time: [09:21:43] training ep: 1 - batch(i): 1050 , loss: 1.3522053956985474\n",">>> time: [09:21:44] training ep: 1 - batch(i): 1060 , loss: 1.3687907457351685\n",">>> time: [09:21:45] training ep: 1 - batch(i): 1070 , loss: 0.795153021812439\n",">>> time: [09:21:46] training ep: 1 - batch(i): 1080 , loss: 1.5790423154830933\n",">>> time: [09:21:47] training ep: 1 - batch(i): 1090 , loss: 1.3251750469207764\n",">>> time: [09:21:47] training ep: 1 - batch(i): 1100 , loss: 1.1109811067581177\n",">>> time: [09:21:48] training ep: 1 - batch(i): 1110 , loss: 1.5464500188827515\n",">>> time: [09:21:49] training ep: 1 - batch(i): 1120 , loss: 0.7040995955467224\n",">>> time: [09:21:50] training ep: 1 - batch(i): 1130 , loss: 1.5594513416290283\n",">>> time: [09:21:51] training ep: 1 - batch(i): 1140 , loss: 0.8926348090171814\n",">>> time: [09:21:52] training ep: 1 - batch(i): 1150 , loss: 2.4618091583251953\n",">>> time: [09:21:53] training ep: 1 - batch(i): 1160 , loss: 0.7709724307060242\n",">>> time: [09:21:54] training ep: 1 - batch(i): 1170 , loss: 0.685260534286499\n",">>> time: [09:21:55] training ep: 1 - batch(i): 1180 , loss: 1.3416869640350342\n",">>> time: [09:21:55] training ep: 1 - batch(i): 1190 , loss: 0.9071640372276306\n",">>> time: [09:21:56] training ep: 1 - batch(i): 1200 , loss: 1.7900645732879639\n",">>> time: [09:21:57] training ep: 1 - batch(i): 1210 , loss: 0.8771969676017761\n",">>> time: [09:21:58] training ep: 1 - batch(i): 1220 , loss: 1.8826600313186646\n",">>> time: [09:21:59] training ep: 1 - batch(i): 1230 , loss: 1.392425298690796\n",">>> time: [09:22:00] training ep: 1 - batch(i): 1240 , loss: 0.9304121136665344\n",">>> time: [09:22:01] training ep: 1 - batch(i): 1250 , loss: 1.464298963546753\n",">>> time: [09:22:02] training ep: 1 - batch(i): 1260 , loss: 1.120191216468811\n",">>> time: [09:22:03] training ep: 1 - batch(i): 1270 , loss: 0.9474164247512817\n",">>> time: [09:22:04] training ep: 1 - batch(i): 1280 , loss: 0.6896030306816101\n",">>> time: [09:22:05] training ep: 1 - batch(i): 1290 , loss: 0.8308184146881104\n",">>> time: [09:22:05] training ep: 1 - batch(i): 1300 , loss: 0.8944323062896729\n",">>> time: [09:22:06] training ep: 1 - batch(i): 1310 , loss: 1.5032994747161865\n",">>> time: [09:22:07] training ep: 1 - batch(i): 1320 , loss: 1.2167383432388306\n",">>> time: [09:22:08] training ep: 1 - batch(i): 1330 , loss: 1.0152256488800049\n",">>> time: [09:22:09] training ep: 1 - batch(i): 1340 , loss: 1.0090446472167969\n",">>> time: [09:22:10] training ep: 1 - batch(i): 1350 , loss: 1.5308058261871338\n",">>> time: [09:22:11] training ep: 1 - batch(i): 1360 , loss: 0.6888686418533325\n",">>> time: [09:22:12] training ep: 1 - batch(i): 1370 , loss: 0.9234496355056763\n",">>> time: [09:22:13] training ep: 1 - batch(i): 1380 , loss: 1.892793893814087\n",">>> time: [09:22:14] training ep: 1 - batch(i): 1390 , loss: 1.0917526483535767\n",">>> time: [09:22:14] training ep: 1 - batch(i): 1400 , loss: 1.45138680934906\n",">>> time: [09:22:15] training ep: 1 - batch(i): 1410 , loss: 1.6000800132751465\n",">>> time: [09:22:16] training ep: 1 - batch(i): 1420 , loss: 0.9697431325912476\n",">>> time: [09:22:17] training ep: 1 - batch(i): 1430 , loss: 2.013179063796997\n",">>> time: [09:22:18] training ep: 1 - batch(i): 1440 , loss: 1.4537193775177002\n",">>> time: [09:22:19] training ep: 1 - batch(i): 1450 , loss: 1.1780834197998047\n",">>> time: [09:22:20] training ep: 1 - batch(i): 1460 , loss: 0.7248915433883667\n",">>> time: [09:22:21] training ep: 1 - batch(i): 1470 , loss: 1.678776741027832\n",">>> time: [09:22:22] training ep: 1 - batch(i): 1480 , loss: 0.850063145160675\n",">>> time: [09:22:22] training ep: 1 - batch(i): 1490 , loss: 1.8606582880020142\n",">>> time: [09:22:23] training ep: 1 - batch(i): 1500 , loss: 1.4660755395889282\n",">>> time: [09:22:24] training ep: 1 - batch(i): 1510 , loss: 1.4121794700622559\n",">>> time: [09:22:25] training ep: 1 - batch(i): 1520 , loss: 0.7753774523735046\n",">>> time: [09:22:26] training ep: 1 - batch(i): 1530 , loss: 2.0274949073791504\n",">>> time: [09:22:27] training ep: 1 - batch(i): 1540 , loss: 0.7877491116523743\n",">>> time: [09:22:28] training ep: 1 - batch(i): 1550 , loss: 1.010047435760498\n",">>> time: [09:22:29] training ep: 1 - batch(i): 1560 , loss: 0.9604363441467285\n",">>> time: [09:22:30] training ep: 1 - batch(i): 1570 , loss: 0.8899264335632324\n",">>> time: [09:22:30] training ep: 1 - batch(i): 1580 , loss: 1.274682879447937\n",">>> time: [09:22:31] training ep: 1 - batch(i): 1590 , loss: 1.0849311351776123\n",">>> time: [09:22:32] training ep: 1 - batch(i): 1600 , loss: 1.5055134296417236\n",">>> time: [09:22:33] training ep: 1 - batch(i): 1610 , loss: 1.0540831089019775\n",">>> time: [09:22:34] training ep: 1 - batch(i): 1620 , loss: 1.6453830003738403\n",">>> time: [09:22:35] training ep: 1 - batch(i): 1630 , loss: 1.3571107387542725\n",">>> time: [09:22:36] training ep: 1 - batch(i): 1640 , loss: 0.8120363354682922\n",">>> time: [09:22:37] training ep: 1 - batch(i): 1650 , loss: 1.1251916885375977\n",">>> time: [09:22:38] training ep: 1 - batch(i): 1660 , loss: 0.9712002277374268\n",">>> time: [09:22:39] training ep: 1 - batch(i): 1670 , loss: 0.7389001846313477\n",">>> time: [09:22:39] training ep: 1 - batch(i): 1680 , loss: 1.038691520690918\n",">>> time: [09:22:40] training ep: 1 - batch(i): 1690 , loss: 0.7228968739509583\n",">>> time: [09:22:41] training ep: 1 - batch(i): 1700 , loss: 0.9012821316719055\n",">>> time: [09:22:42] training ep: 1 - batch(i): 1710 , loss: 0.6797389984130859\n",">>> time: [09:22:43] training ep: 1 - batch(i): 1720 , loss: 1.0724780559539795\n",">>> time: [09:22:44] training ep: 1 - batch(i): 1730 , loss: 1.4490066766738892\n",">>> time: [09:22:45] training ep: 1 - batch(i): 1740 , loss: 0.9269970059394836\n",">>> time: [09:22:46] training ep: 1 - batch(i): 1750 , loss: 0.7127901315689087\n",">>> time: [09:22:47] training ep: 1 - batch(i): 1760 , loss: 0.722565770149231\n",">>> time: [09:22:47] training ep: 1 - batch(i): 1770 , loss: 1.454141616821289\n",">>> time: [09:22:48] training ep: 1 - batch(i): 1780 , loss: 0.7529140710830688\n",">>> time: [09:22:49] training ep: 1 - batch(i): 1790 , loss: 0.8428378701210022\n",">>> time: [09:22:50] training ep: 1 - batch(i): 1800 , loss: 0.9310954809188843\n",">>> time: [09:22:51] training ep: 1 - batch(i): 1810 , loss: 1.2676200866699219\n",">>> time: [09:22:52] training ep: 1 - batch(i): 1820 , loss: 1.280938982963562\n",">>> time: [09:22:53] training ep: 1 - batch(i): 1830 , loss: 0.9361987113952637\n",">>> time: [09:22:54] training ep: 1 - batch(i): 1840 , loss: 1.6276023387908936\n",">>> time: [09:22:55] training ep: 1 - batch(i): 1850 , loss: 0.8242598176002502\n",">>> time: [09:22:56] training ep: 1 - batch(i): 1860 , loss: 0.7609437704086304\n",">>> time: [09:22:56] training ep: 1 - batch(i): 1870 , loss: 1.6891084909439087\n",">>> time: [09:22:57] training ep: 1 - batch(i): 1880 , loss: 1.0721598863601685\n",">>> time: [09:22:58] training ep: 1 - batch(i): 1890 , loss: 0.7228882312774658\n",">>> time: [09:22:59] training ep: 1 - batch(i): 1900 , loss: 1.0630152225494385\n",">>> time: [09:23:00] training ep: 1 - batch(i): 1910 , loss: 0.8412033915519714\n",">>> time: [09:23:01] training ep: 1 - batch(i): 1920 , loss: 0.7817843556404114\n",">>> time: [09:23:02] training ep: 1 - batch(i): 1930 , loss: 1.4753142595291138\n",">>> time: [09:23:03] training ep: 1 - batch(i): 1940 , loss: 0.756672203540802\n",">>> time: [09:23:04] training ep: 1 - batch(i): 1950 , loss: 0.9164137244224548\n",">>> time: [09:23:04] training ep: 1 - batch(i): 1960 , loss: 1.2366544008255005\n",">>> time: [09:23:05] training ep: 1 - batch(i): 1970 , loss: 0.9284077882766724\n",">>> time: [09:23:06] training ep: 1 - batch(i): 1980 , loss: 0.9954240918159485\n",">>> time: [09:23:07] training ep: 1 - batch(i): 1990 , loss: 0.6388062834739685\n",">>> time: [09:23:08] training ep: 1 - batch(i): 2000 , loss: 0.9634785056114197\n",">>> time: [09:23:09] training ep: 1 - batch(i): 2010 , loss: 0.4451944828033447\n",">>> time: [09:23:10] training ep: 1 - batch(i): 2020 , loss: 1.109248399734497\n",">>> time: [09:23:11] training ep: 1 - batch(i): 2030 , loss: 1.6400518417358398\n",">>> time: [09:23:12] training ep: 1 - batch(i): 2040 , loss: 0.6073148250579834\n",">>> time: [09:23:12] training ep: 1 - batch(i): 2050 , loss: 1.1471461057662964\n",">>> time: [09:23:13] training ep: 1 - batch(i): 2060 , loss: 0.9801415205001831\n",">>> time: [09:23:14] training ep: 1 - batch(i): 2070 , loss: 0.7952827215194702\n",">>> time: [09:23:15] training ep: 1 - batch(i): 2080 , loss: 0.9793199896812439\n",">>> time: [09:23:16] training ep: 1 - batch(i): 2090 , loss: 0.5689958333969116\n",">>> time: [09:23:17] training ep: 1 - batch(i): 2100 , loss: 1.2416936159133911\n",">>> time: [09:23:18] training ep: 1 - batch(i): 2110 , loss: 0.8013047575950623\n",">>> time: [09:23:19] training ep: 1 - batch(i): 2120 , loss: 0.9072681069374084\n",">>> time: [09:23:19] training ep: 1 - batch(i): 2130 , loss: 0.6407080888748169\n",">>> time: [09:23:20] training ep: 1 - batch(i): 2140 , loss: 1.129006028175354\n",">>> time: [09:23:21] training ep: 1 - batch(i): 2150 , loss: 0.7328401207923889\n",">>> time: [09:23:22] training ep: 1 - batch(i): 2160 , loss: 0.6176510453224182\n",">>> time: [09:23:23] training ep: 1 - batch(i): 2170 , loss: 1.1227831840515137\n",">>> time: [09:23:24] training ep: 1 - batch(i): 2180 , loss: 0.5346814393997192\n",">>> time: [09:23:25] training ep: 1 - batch(i): 2190 , loss: 0.8199625015258789\n",">>> time: [09:23:26] training ep: 1 - batch(i): 2200 , loss: 0.8393043875694275\n",">>> time: [09:23:26] training ep: 1 - batch(i): 2210 , loss: 0.8229800462722778\n",">>> time: [09:23:27] training ep: 1 - batch(i): 2220 , loss: 0.9824833273887634\n",">>> time: [09:23:28] training ep: 1 - batch(i): 2230 , loss: 0.5864234566688538\n",">>> time: [09:23:29] training ep: 1 - batch(i): 2240 , loss: 0.9011148810386658\n","training ep 2 - trainlen: 2242\n",">>> time: [09:23:31] training ep: 2 - batch(i): 0 , loss: 0.5875868201255798\n",">>> time: [09:23:32] training ep: 2 - batch(i): 10 , loss: 0.9405658841133118\n",">>> time: [09:23:33] training ep: 2 - batch(i): 20 , loss: 0.9406391978263855\n",">>> time: [09:23:33] training ep: 2 - batch(i): 30 , loss: 0.8629769086837769\n",">>> time: [09:23:34] training ep: 2 - batch(i): 40 , loss: 0.6394928097724915\n",">>> time: [09:23:35] training ep: 2 - batch(i): 50 , loss: 1.4264262914657593\n",">>> time: [09:23:36] training ep: 2 - batch(i): 60 , loss: 0.6659354567527771\n",">>> time: [09:23:37] training ep: 2 - batch(i): 70 , loss: 1.1794016361236572\n",">>> time: [09:23:38] training ep: 2 - batch(i): 80 , loss: 1.1262413263320923\n",">>> time: [09:23:39] training ep: 2 - batch(i): 90 , loss: 1.3707342147827148\n",">>> time: [09:23:40] training ep: 2 - batch(i): 100 , loss: 0.611621081829071\n",">>> time: [09:23:41] training ep: 2 - batch(i): 110 , loss: 1.2258449792861938\n",">>> time: [09:23:42] training ep: 2 - batch(i): 120 , loss: 1.0272945165634155\n",">>> time: [09:23:42] training ep: 2 - batch(i): 130 , loss: 1.0935951471328735\n",">>> time: [09:23:43] training ep: 2 - batch(i): 140 , loss: 1.1021064519882202\n",">>> time: [09:23:44] training ep: 2 - batch(i): 150 , loss: 1.3525640964508057\n",">>> time: [09:23:45] training ep: 2 - batch(i): 160 , loss: 1.122363805770874\n",">>> time: [09:23:46] training ep: 2 - batch(i): 170 , loss: 1.2085039615631104\n",">>> time: [09:23:47] training ep: 2 - batch(i): 180 , loss: 1.596049189567566\n",">>> time: [09:23:48] training ep: 2 - batch(i): 190 , loss: 1.3186284303665161\n",">>> time: [09:23:49] training ep: 2 - batch(i): 200 , loss: 1.059503197669983\n",">>> time: [09:23:50] training ep: 2 - batch(i): 210 , loss: 0.8738933205604553\n",">>> time: [09:23:51] training ep: 2 - batch(i): 220 , loss: 0.9714030623435974\n",">>> time: [09:23:52] training ep: 2 - batch(i): 230 , loss: 1.8767560720443726\n",">>> time: [09:23:52] training ep: 2 - batch(i): 240 , loss: 1.8042703866958618\n",">>> time: [09:23:53] training ep: 2 - batch(i): 250 , loss: 0.9720353484153748\n",">>> time: [09:23:54] training ep: 2 - batch(i): 260 , loss: 0.9606926441192627\n",">>> time: [09:23:55] training ep: 2 - batch(i): 270 , loss: 1.7215818166732788\n",">>> time: [09:23:56] training ep: 2 - batch(i): 280 , loss: 0.6987835168838501\n",">>> time: [09:23:57] training ep: 2 - batch(i): 290 , loss: 0.4990321397781372\n",">>> time: [09:23:58] training ep: 2 - batch(i): 300 , loss: 1.3875659704208374\n",">>> time: [09:23:59] training ep: 2 - batch(i): 310 , loss: 0.7441754341125488\n",">>> time: [09:24:00] training ep: 2 - batch(i): 320 , loss: 0.8149371147155762\n",">>> time: [09:24:01] training ep: 2 - batch(i): 330 , loss: 0.7549092769622803\n",">>> time: [09:24:02] training ep: 2 - batch(i): 340 , loss: 1.0195075273513794\n",">>> time: [09:24:02] training ep: 2 - batch(i): 350 , loss: 0.9318445324897766\n",">>> time: [09:24:03] training ep: 2 - batch(i): 360 , loss: 0.5699756741523743\n",">>> time: [09:24:04] training ep: 2 - batch(i): 370 , loss: 0.858536958694458\n",">>> time: [09:24:05] training ep: 2 - batch(i): 380 , loss: 1.1897995471954346\n",">>> time: [09:24:06] training ep: 2 - batch(i): 390 , loss: 1.4803451299667358\n",">>> time: [09:24:07] training ep: 2 - batch(i): 400 , loss: 1.4069281816482544\n",">>> time: [09:24:08] training ep: 2 - batch(i): 410 , loss: 0.6749045252799988\n",">>> time: [09:24:09] training ep: 2 - batch(i): 420 , loss: 1.3448059558868408\n",">>> time: [09:24:10] training ep: 2 - batch(i): 430 , loss: 0.7540205717086792\n",">>> time: [09:24:11] training ep: 2 - batch(i): 440 , loss: 1.281765103340149\n",">>> time: [09:24:12] training ep: 2 - batch(i): 450 , loss: 0.7996553778648376\n",">>> time: [09:24:13] training ep: 2 - batch(i): 460 , loss: 0.6738284230232239\n",">>> time: [09:24:14] training ep: 2 - batch(i): 470 , loss: 1.9015785455703735\n",">>> time: [09:24:14] training ep: 2 - batch(i): 480 , loss: 1.20333993434906\n",">>> time: [09:24:15] training ep: 2 - batch(i): 490 , loss: 0.6940512657165527\n",">>> time: [09:24:16] training ep: 2 - batch(i): 500 , loss: 1.8595534563064575\n",">>> time: [09:24:17] training ep: 2 - batch(i): 510 , loss: 1.1905070543289185\n",">>> time: [09:24:18] training ep: 2 - batch(i): 520 , loss: 1.119619607925415\n",">>> time: [09:24:19] training ep: 2 - batch(i): 530 , loss: 0.8394019603729248\n",">>> time: [09:24:20] training ep: 2 - batch(i): 540 , loss: 0.8194295167922974\n",">>> time: [09:24:21] training ep: 2 - batch(i): 550 , loss: 1.315308928489685\n",">>> time: [09:24:22] training ep: 2 - batch(i): 560 , loss: 1.1271493434906006\n",">>> time: [09:24:23] training ep: 2 - batch(i): 570 , loss: 0.8279237151145935\n",">>> time: [09:24:24] training ep: 2 - batch(i): 580 , loss: 1.6381806135177612\n",">>> time: [09:24:25] training ep: 2 - batch(i): 590 , loss: 1.0983092784881592\n",">>> time: [09:24:25] training ep: 2 - batch(i): 600 , loss: 0.6690605282783508\n",">>> time: [09:24:26] training ep: 2 - batch(i): 610 , loss: 0.7547500133514404\n",">>> time: [09:24:27] training ep: 2 - batch(i): 620 , loss: 1.0149061679840088\n",">>> time: [09:24:28] training ep: 2 - batch(i): 630 , loss: 0.6684618592262268\n",">>> time: [09:24:29] training ep: 2 - batch(i): 640 , loss: 0.5766438841819763\n",">>> time: [09:24:30] training ep: 2 - batch(i): 650 , loss: 1.3349817991256714\n",">>> time: [09:24:31] training ep: 2 - batch(i): 660 , loss: 0.7693597674369812\n",">>> time: [09:24:32] training ep: 2 - batch(i): 670 , loss: 1.0243442058563232\n",">>> time: [09:24:33] training ep: 2 - batch(i): 680 , loss: 1.620732307434082\n",">>> time: [09:24:34] training ep: 2 - batch(i): 690 , loss: 1.3427096605300903\n",">>> time: [09:24:35] training ep: 2 - batch(i): 700 , loss: 1.0273656845092773\n",">>> time: [09:24:35] training ep: 2 - batch(i): 710 , loss: 0.9540721774101257\n",">>> time: [09:24:36] training ep: 2 - batch(i): 720 , loss: 0.8908079266548157\n",">>> time: [09:24:37] training ep: 2 - batch(i): 730 , loss: 0.6402332186698914\n",">>> time: [09:24:38] training ep: 2 - batch(i): 740 , loss: 1.2812731266021729\n",">>> time: [09:24:39] training ep: 2 - batch(i): 750 , loss: 0.6637097597122192\n",">>> time: [09:24:40] training ep: 2 - batch(i): 760 , loss: 1.5155906677246094\n",">>> time: [09:24:41] training ep: 2 - batch(i): 770 , loss: 0.7250401377677917\n",">>> time: [09:24:42] training ep: 2 - batch(i): 780 , loss: 2.129849910736084\n",">>> time: [09:24:43] training ep: 2 - batch(i): 790 , loss: 1.2404855489730835\n",">>> time: [09:24:44] training ep: 2 - batch(i): 800 , loss: 1.674740195274353\n",">>> time: [09:24:45] training ep: 2 - batch(i): 810 , loss: 1.082010269165039\n",">>> time: [09:24:45] training ep: 2 - batch(i): 820 , loss: 0.7940442562103271\n",">>> time: [09:24:46] training ep: 2 - batch(i): 830 , loss: 0.8661112189292908\n",">>> time: [09:24:47] training ep: 2 - batch(i): 840 , loss: 1.1094012260437012\n",">>> time: [09:24:48] training ep: 2 - batch(i): 850 , loss: 1.2791105508804321\n",">>> time: [09:24:49] training ep: 2 - batch(i): 860 , loss: 0.517788827419281\n",">>> time: [09:24:50] training ep: 2 - batch(i): 870 , loss: 0.657798707485199\n",">>> time: [09:24:51] training ep: 2 - batch(i): 880 , loss: 0.8391308784484863\n",">>> time: [09:24:52] training ep: 2 - batch(i): 890 , loss: 0.3256927728652954\n",">>> time: [09:24:53] training ep: 2 - batch(i): 900 , loss: 0.7240038514137268\n",">>> time: [09:24:54] training ep: 2 - batch(i): 910 , loss: 0.9637569189071655\n",">>> time: [09:24:54] training ep: 2 - batch(i): 920 , loss: 0.9884393811225891\n",">>> time: [09:24:55] training ep: 2 - batch(i): 930 , loss: 1.5070072412490845\n",">>> time: [09:24:56] training ep: 2 - batch(i): 940 , loss: 0.958728551864624\n",">>> time: [09:24:57] training ep: 2 - batch(i): 950 , loss: 0.6005584597587585\n",">>> time: [09:24:58] training ep: 2 - batch(i): 960 , loss: 0.7369387745857239\n",">>> time: [09:24:59] training ep: 2 - batch(i): 970 , loss: 0.8716521263122559\n",">>> time: [09:25:00] training ep: 2 - batch(i): 980 , loss: 1.1345500946044922\n",">>> time: [09:25:01] training ep: 2 - batch(i): 990 , loss: 1.3952564001083374\n",">>> time: [09:25:02] training ep: 2 - batch(i): 1000 , loss: 0.8981985449790955\n",">>> time: [09:25:03] training ep: 2 - batch(i): 1010 , loss: 1.138616919517517\n",">>> time: [09:25:04] training ep: 2 - batch(i): 1020 , loss: 0.9841806888580322\n",">>> time: [09:25:04] training ep: 2 - batch(i): 1030 , loss: 0.7848004698753357\n",">>> time: [09:25:05] training ep: 2 - batch(i): 1040 , loss: 0.77407306432724\n",">>> time: [09:25:06] training ep: 2 - batch(i): 1050 , loss: 1.5314180850982666\n",">>> time: [09:25:07] training ep: 2 - batch(i): 1060 , loss: 0.9103803634643555\n",">>> time: [09:25:08] training ep: 2 - batch(i): 1070 , loss: 0.6725866198539734\n",">>> time: [09:25:09] training ep: 2 - batch(i): 1080 , loss: 0.7968617677688599\n",">>> time: [09:25:10] training ep: 2 - batch(i): 1090 , loss: 0.725363552570343\n",">>> time: [09:25:11] training ep: 2 - batch(i): 1100 , loss: 0.8348912000656128\n",">>> time: [09:25:12] training ep: 2 - batch(i): 1110 , loss: 0.46300598978996277\n",">>> time: [09:25:13] training ep: 2 - batch(i): 1120 , loss: 0.9020991325378418\n",">>> time: [09:25:13] training ep: 2 - batch(i): 1130 , loss: 1.0321323871612549\n",">>> time: [09:25:14] training ep: 2 - batch(i): 1140 , loss: 0.9638910293579102\n",">>> time: [09:25:15] training ep: 2 - batch(i): 1150 , loss: 1.072151780128479\n",">>> time: [09:25:16] training ep: 2 - batch(i): 1160 , loss: 1.0349745750427246\n",">>> time: [09:25:17] training ep: 2 - batch(i): 1170 , loss: 0.8601224422454834\n",">>> time: [09:25:18] training ep: 2 - batch(i): 1180 , loss: 0.7809170484542847\n",">>> time: [09:25:19] training ep: 2 - batch(i): 1190 , loss: 1.084486961364746\n",">>> time: [09:25:20] training ep: 2 - batch(i): 1200 , loss: 1.1251460313796997\n",">>> time: [09:25:21] training ep: 2 - batch(i): 1210 , loss: 1.7784709930419922\n",">>> time: [09:25:22] training ep: 2 - batch(i): 1220 , loss: 0.7229562401771545\n",">>> time: [09:25:22] training ep: 2 - batch(i): 1230 , loss: 0.7104744911193848\n",">>> time: [09:25:23] training ep: 2 - batch(i): 1240 , loss: 0.6920903325080872\n",">>> time: [09:25:24] training ep: 2 - batch(i): 1250 , loss: 0.6239587664604187\n",">>> time: [09:25:25] training ep: 2 - batch(i): 1260 , loss: 1.2563139200210571\n",">>> time: [09:25:26] training ep: 2 - batch(i): 1270 , loss: 0.8269473910331726\n",">>> time: [09:25:27] training ep: 2 - batch(i): 1280 , loss: 0.6102639436721802\n",">>> time: [09:25:28] training ep: 2 - batch(i): 1290 , loss: 1.514115571975708\n",">>> time: [09:25:29] training ep: 2 - batch(i): 1300 , loss: 1.0988115072250366\n",">>> time: [09:25:30] training ep: 2 - batch(i): 1310 , loss: 0.5959473252296448\n",">>> time: [09:25:31] training ep: 2 - batch(i): 1320 , loss: 0.8417654037475586\n",">>> time: [09:25:31] training ep: 2 - batch(i): 1330 , loss: 1.1766903400421143\n",">>> time: [09:25:32] training ep: 2 - batch(i): 1340 , loss: 0.5794141888618469\n",">>> time: [09:25:33] training ep: 2 - batch(i): 1350 , loss: 0.6705705523490906\n",">>> time: [09:25:34] training ep: 2 - batch(i): 1360 , loss: 0.6189028024673462\n",">>> time: [09:25:35] training ep: 2 - batch(i): 1370 , loss: 0.8938921093940735\n",">>> time: [09:25:36] training ep: 2 - batch(i): 1380 , loss: 0.8198367953300476\n",">>> time: [09:25:37] training ep: 2 - batch(i): 1390 , loss: 1.6686040163040161\n",">>> time: [09:25:38] training ep: 2 - batch(i): 1400 , loss: 1.0211926698684692\n",">>> time: [09:25:39] training ep: 2 - batch(i): 1410 , loss: 1.0434235334396362\n",">>> time: [09:25:40] training ep: 2 - batch(i): 1420 , loss: 0.8892632722854614\n",">>> time: [09:25:41] training ep: 2 - batch(i): 1430 , loss: 0.6443496942520142\n",">>> time: [09:25:41] training ep: 2 - batch(i): 1440 , loss: 0.7091012597084045\n",">>> time: [09:25:42] training ep: 2 - batch(i): 1450 , loss: 0.7762323617935181\n",">>> time: [09:25:43] training ep: 2 - batch(i): 1460 , loss: 0.9862591028213501\n",">>> time: [09:25:44] training ep: 2 - batch(i): 1470 , loss: 0.5872060060501099\n",">>> time: [09:25:45] training ep: 2 - batch(i): 1480 , loss: 0.6013861894607544\n",">>> time: [09:25:46] training ep: 2 - batch(i): 1490 , loss: 1.1052550077438354\n",">>> time: [09:25:47] training ep: 2 - batch(i): 1500 , loss: 0.8072745203971863\n",">>> time: [09:25:48] training ep: 2 - batch(i): 1510 , loss: 0.41601458191871643\n",">>> time: [09:25:49] training ep: 2 - batch(i): 1520 , loss: 1.2001922130584717\n",">>> time: [09:25:50] training ep: 2 - batch(i): 1530 , loss: 1.0560709238052368\n",">>> time: [09:25:50] training ep: 2 - batch(i): 1540 , loss: 0.9686173796653748\n",">>> time: [09:25:51] training ep: 2 - batch(i): 1550 , loss: 1.1616978645324707\n",">>> time: [09:25:52] training ep: 2 - batch(i): 1560 , loss: 1.6410411596298218\n",">>> time: [09:25:53] training ep: 2 - batch(i): 1570 , loss: 0.5068092346191406\n",">>> time: [09:25:54] training ep: 2 - batch(i): 1580 , loss: 0.4926365315914154\n",">>> time: [09:25:55] training ep: 2 - batch(i): 1590 , loss: 0.9870918989181519\n",">>> time: [09:25:56] training ep: 2 - batch(i): 1600 , loss: 1.3647396564483643\n",">>> time: [09:25:57] training ep: 2 - batch(i): 1610 , loss: 0.6745771169662476\n",">>> time: [09:25:58] training ep: 2 - batch(i): 1620 , loss: 0.5200951099395752\n",">>> time: [09:25:59] training ep: 2 - batch(i): 1630 , loss: 0.6833978891372681\n",">>> time: [09:26:00] training ep: 2 - batch(i): 1640 , loss: 0.8662837147712708\n",">>> time: [09:26:00] training ep: 2 - batch(i): 1650 , loss: 1.6448131799697876\n",">>> time: [09:26:01] training ep: 2 - batch(i): 1660 , loss: 0.9819138050079346\n",">>> time: [09:26:02] training ep: 2 - batch(i): 1670 , loss: 0.8301554322242737\n",">>> time: [09:26:03] training ep: 2 - batch(i): 1680 , loss: 1.5539628267288208\n",">>> time: [09:26:04] training ep: 2 - batch(i): 1690 , loss: 0.46170225739479065\n",">>> time: [09:26:05] training ep: 2 - batch(i): 1700 , loss: 0.8579692840576172\n",">>> time: [09:26:06] training ep: 2 - batch(i): 1710 , loss: 0.6231414079666138\n",">>> time: [09:26:07] training ep: 2 - batch(i): 1720 , loss: 0.597158670425415\n",">>> time: [09:26:08] training ep: 2 - batch(i): 1730 , loss: 0.7115467190742493\n",">>> time: [09:26:09] training ep: 2 - batch(i): 1740 , loss: 0.6844565868377686\n",">>> time: [09:26:10] training ep: 2 - batch(i): 1750 , loss: 0.6960492134094238\n",">>> time: [09:26:10] training ep: 2 - batch(i): 1760 , loss: 1.0175971984863281\n",">>> time: [09:26:11] training ep: 2 - batch(i): 1770 , loss: 1.1940994262695312\n",">>> time: [09:26:12] training ep: 2 - batch(i): 1780 , loss: 0.5266667008399963\n",">>> time: [09:26:13] training ep: 2 - batch(i): 1790 , loss: 0.7237313389778137\n",">>> time: [09:26:14] training ep: 2 - batch(i): 1800 , loss: 0.5471802353858948\n",">>> time: [09:26:15] training ep: 2 - batch(i): 1810 , loss: 0.8644130825996399\n",">>> time: [09:26:16] training ep: 2 - batch(i): 1820 , loss: 0.502816379070282\n",">>> time: [09:26:17] training ep: 2 - batch(i): 1830 , loss: 0.45527729392051697\n",">>> time: [09:26:18] training ep: 2 - batch(i): 1840 , loss: 0.6319107413291931\n",">>> time: [09:26:19] training ep: 2 - batch(i): 1850 , loss: 0.4140913188457489\n",">>> time: [09:26:20] training ep: 2 - batch(i): 1860 , loss: 0.4574480950832367\n",">>> time: [09:26:20] training ep: 2 - batch(i): 1870 , loss: 0.9044426083564758\n",">>> time: [09:26:21] training ep: 2 - batch(i): 1880 , loss: 0.5091517567634583\n",">>> time: [09:26:22] training ep: 2 - batch(i): 1890 , loss: 0.8450062274932861\n",">>> time: [09:26:23] training ep: 2 - batch(i): 1900 , loss: 0.49789464473724365\n",">>> time: [09:26:24] training ep: 2 - batch(i): 1910 , loss: 0.4877406656742096\n",">>> time: [09:26:25] training ep: 2 - batch(i): 1920 , loss: 0.5405142903327942\n",">>> time: [09:26:26] training ep: 2 - batch(i): 1930 , loss: 0.8009698987007141\n",">>> time: [09:26:27] training ep: 2 - batch(i): 1940 , loss: 0.3148675262928009\n",">>> time: [09:26:28] training ep: 2 - batch(i): 1950 , loss: 0.7895269989967346\n",">>> time: [09:26:29] training ep: 2 - batch(i): 1960 , loss: 0.5991265177726746\n",">>> time: [09:26:30] training ep: 2 - batch(i): 1970 , loss: 0.674847424030304\n",">>> time: [09:26:30] training ep: 2 - batch(i): 1980 , loss: 0.46459630131721497\n",">>> time: [09:26:31] training ep: 2 - batch(i): 1990 , loss: 0.7119766473770142\n",">>> time: [09:26:32] training ep: 2 - batch(i): 2000 , loss: 1.1160486936569214\n",">>> time: [09:26:33] training ep: 2 - batch(i): 2010 , loss: 0.8373245596885681\n",">>> time: [09:26:34] training ep: 2 - batch(i): 2020 , loss: 0.6840548515319824\n",">>> time: [09:26:35] training ep: 2 - batch(i): 2030 , loss: 0.5803167223930359\n",">>> time: [09:26:36] training ep: 2 - batch(i): 2040 , loss: 1.0455923080444336\n",">>> time: [09:26:37] training ep: 2 - batch(i): 2050 , loss: 0.5266826152801514\n",">>> time: [09:26:38] training ep: 2 - batch(i): 2060 , loss: 0.8345474600791931\n",">>> time: [09:26:39] training ep: 2 - batch(i): 2070 , loss: 0.7278286218643188\n",">>> time: [09:26:40] training ep: 2 - batch(i): 2080 , loss: 0.6878288984298706\n",">>> time: [09:26:40] training ep: 2 - batch(i): 2090 , loss: 0.5178407430648804\n",">>> time: [09:26:41] training ep: 2 - batch(i): 2100 , loss: 0.5299479365348816\n",">>> time: [09:26:42] training ep: 2 - batch(i): 2110 , loss: 0.8936001062393188\n",">>> time: [09:26:43] training ep: 2 - batch(i): 2120 , loss: 0.89461350440979\n",">>> time: [09:26:44] training ep: 2 - batch(i): 2130 , loss: 0.9412605166435242\n",">>> time: [09:26:45] training ep: 2 - batch(i): 2140 , loss: 0.44133883714675903\n",">>> time: [09:26:46] training ep: 2 - batch(i): 2150 , loss: 0.6134106516838074\n",">>> time: [09:26:47] training ep: 2 - batch(i): 2160 , loss: 0.8839443922042847\n",">>> time: [09:26:48] training ep: 2 - batch(i): 2170 , loss: 0.5834301710128784\n",">>> time: [09:26:49] training ep: 2 - batch(i): 2180 , loss: 0.7343779802322388\n",">>> time: [09:26:50] training ep: 2 - batch(i): 2190 , loss: 0.6186951398849487\n",">>> time: [09:26:50] training ep: 2 - batch(i): 2200 , loss: 0.6209099292755127\n",">>> time: [09:26:51] training ep: 2 - batch(i): 2210 , loss: 0.9134534001350403\n",">>> time: [09:26:52] training ep: 2 - batch(i): 2220 , loss: 0.8497698903083801\n",">>> time: [09:26:53] training ep: 2 - batch(i): 2230 , loss: 0.5414188504219055\n",">>> time: [09:26:54] training ep: 2 - batch(i): 2240 , loss: 0.8535032868385315\n","training ep 3 - trainlen: 2242\n",">>> time: [09:26:56] training ep: 3 - batch(i): 0 , loss: 0.5695642828941345\n",">>> time: [09:26:57] training ep: 3 - batch(i): 10 , loss: 0.5350789427757263\n",">>> time: [09:26:58] training ep: 3 - batch(i): 20 , loss: 0.7318263649940491\n",">>> time: [09:26:59] training ep: 3 - batch(i): 30 , loss: 0.9048587083816528\n",">>> time: [09:27:00] training ep: 3 - batch(i): 40 , loss: 0.738288402557373\n",">>> time: [09:27:01] training ep: 3 - batch(i): 50 , loss: 0.6346321702003479\n",">>> time: [09:27:02] training ep: 3 - batch(i): 60 , loss: 0.624911904335022\n",">>> time: [09:27:02] training ep: 3 - batch(i): 70 , loss: 1.0102028846740723\n",">>> time: [09:27:03] training ep: 3 - batch(i): 80 , loss: 1.2051188945770264\n",">>> time: [09:27:04] training ep: 3 - batch(i): 90 , loss: 0.9605057835578918\n",">>> time: [09:27:05] training ep: 3 - batch(i): 100 , loss: 0.49415016174316406\n",">>> time: [09:27:06] training ep: 3 - batch(i): 110 , loss: 0.9915419816970825\n",">>> time: [09:27:07] training ep: 3 - batch(i): 120 , loss: 0.5402565002441406\n",">>> time: [09:27:08] training ep: 3 - batch(i): 130 , loss: 0.6648018956184387\n",">>> time: [09:27:09] training ep: 3 - batch(i): 140 , loss: 0.800500214099884\n",">>> time: [09:27:10] training ep: 3 - batch(i): 150 , loss: 1.3014801740646362\n",">>> time: [09:27:11] training ep: 3 - batch(i): 160 , loss: 0.8307176828384399\n",">>> time: [09:27:12] training ep: 3 - batch(i): 170 , loss: 0.8347548246383667\n",">>> time: [09:27:13] training ep: 3 - batch(i): 180 , loss: 0.37638524174690247\n",">>> time: [09:27:14] training ep: 3 - batch(i): 190 , loss: 0.7328621745109558\n",">>> time: [09:27:15] training ep: 3 - batch(i): 200 , loss: 0.6849688291549683\n",">>> time: [09:27:15] training ep: 3 - batch(i): 210 , loss: 0.8367096185684204\n",">>> time: [09:27:16] training ep: 3 - batch(i): 220 , loss: 0.6310878992080688\n",">>> time: [09:27:17] training ep: 3 - batch(i): 230 , loss: 0.4394528269767761\n",">>> time: [09:27:18] training ep: 3 - batch(i): 240 , loss: 0.9820699095726013\n",">>> time: [09:27:19] training ep: 3 - batch(i): 250 , loss: 0.7713300585746765\n",">>> time: [09:27:20] training ep: 3 - batch(i): 260 , loss: 0.6248701810836792\n",">>> time: [09:27:21] training ep: 3 - batch(i): 270 , loss: 0.782823383808136\n",">>> time: [09:27:22] training ep: 3 - batch(i): 280 , loss: 0.7282695174217224\n",">>> time: [09:27:23] training ep: 3 - batch(i): 290 , loss: 1.0220659971237183\n",">>> time: [09:27:24] training ep: 3 - batch(i): 300 , loss: 0.7276566028594971\n",">>> time: [09:27:24] training ep: 3 - batch(i): 310 , loss: 0.8070002198219299\n",">>> time: [09:27:25] training ep: 3 - batch(i): 320 , loss: 0.8188171982765198\n",">>> time: [09:27:26] training ep: 3 - batch(i): 330 , loss: 0.9135657548904419\n",">>> time: [09:27:27] training ep: 3 - batch(i): 340 , loss: 0.46991637349128723\n",">>> time: [09:27:28] training ep: 3 - batch(i): 350 , loss: 0.4558826982975006\n",">>> time: [09:27:29] training ep: 3 - batch(i): 360 , loss: 0.9101530313491821\n",">>> time: [09:27:30] training ep: 3 - batch(i): 370 , loss: 0.7842289209365845\n",">>> time: [09:27:31] training ep: 3 - batch(i): 380 , loss: 0.8580710887908936\n",">>> time: [09:27:32] training ep: 3 - batch(i): 390 , loss: 0.38924142718315125\n",">>> time: [09:27:33] training ep: 3 - batch(i): 400 , loss: 0.7831529378890991\n",">>> time: [09:27:34] training ep: 3 - batch(i): 410 , loss: 0.7837969660758972\n",">>> time: [09:27:35] training ep: 3 - batch(i): 420 , loss: 1.0285496711730957\n",">>> time: [09:27:35] training ep: 3 - batch(i): 430 , loss: 0.585669994354248\n",">>> time: [09:27:36] training ep: 3 - batch(i): 440 , loss: 0.7210274934768677\n",">>> time: [09:27:37] training ep: 3 - batch(i): 450 , loss: 0.6369572877883911\n",">>> time: [09:27:38] training ep: 3 - batch(i): 460 , loss: 0.6151922345161438\n",">>> time: [09:27:39] training ep: 3 - batch(i): 470 , loss: 0.7881776690483093\n",">>> time: [09:27:40] training ep: 3 - batch(i): 480 , loss: 0.5531998872756958\n",">>> time: [09:27:41] training ep: 3 - batch(i): 490 , loss: 0.9616661071777344\n",">>> time: [09:27:42] training ep: 3 - batch(i): 500 , loss: 0.5475113987922668\n",">>> time: [09:27:43] training ep: 3 - batch(i): 510 , loss: 1.4327377080917358\n",">>> time: [09:27:44] training ep: 3 - batch(i): 520 , loss: 0.7131630778312683\n",">>> time: [09:27:45] training ep: 3 - batch(i): 530 , loss: 0.7677821516990662\n",">>> time: [09:27:45] training ep: 3 - batch(i): 540 , loss: 0.676392674446106\n",">>> time: [09:27:46] training ep: 3 - batch(i): 550 , loss: 0.41404494643211365\n",">>> time: [09:27:47] training ep: 3 - batch(i): 560 , loss: 0.5066958665847778\n",">>> time: [09:27:48] training ep: 3 - batch(i): 570 , loss: 0.4555286467075348\n",">>> time: [09:27:49] training ep: 3 - batch(i): 580 , loss: 0.6452080011367798\n",">>> time: [09:27:50] training ep: 3 - batch(i): 590 , loss: 0.6355533003807068\n",">>> time: [09:27:51] training ep: 3 - batch(i): 600 , loss: 0.7618497610092163\n",">>> time: [09:27:52] training ep: 3 - batch(i): 610 , loss: 0.4481756091117859\n",">>> time: [09:27:53] training ep: 3 - batch(i): 620 , loss: 1.2307926416397095\n",">>> time: [09:27:54] training ep: 3 - batch(i): 630 , loss: 0.8346518874168396\n",">>> time: [09:27:54] training ep: 3 - batch(i): 640 , loss: 0.5550282597541809\n",">>> time: [09:27:55] training ep: 3 - batch(i): 650 , loss: 0.35663118958473206\n",">>> time: [09:27:56] training ep: 3 - batch(i): 660 , loss: 0.6692182421684265\n",">>> time: [09:27:57] training ep: 3 - batch(i): 670 , loss: 0.8099638223648071\n",">>> time: [09:27:58] training ep: 3 - batch(i): 680 , loss: 0.4443104863166809\n",">>> time: [09:27:59] training ep: 3 - batch(i): 690 , loss: 0.9608445167541504\n",">>> time: [09:28:00] training ep: 3 - batch(i): 700 , loss: 0.5667104721069336\n",">>> time: [09:28:01] training ep: 3 - batch(i): 710 , loss: 0.43479928374290466\n",">>> time: [09:28:02] training ep: 3 - batch(i): 720 , loss: 0.6556958556175232\n",">>> time: [09:28:03] training ep: 3 - batch(i): 730 , loss: 1.399638295173645\n",">>> time: [09:28:04] training ep: 3 - batch(i): 740 , loss: 1.0502347946166992\n",">>> time: [09:28:04] training ep: 3 - batch(i): 750 , loss: 1.2456623315811157\n",">>> time: [09:28:05] training ep: 3 - batch(i): 760 , loss: 1.0083255767822266\n",">>> time: [09:28:06] training ep: 3 - batch(i): 770 , loss: 0.7996809482574463\n",">>> time: [09:28:07] training ep: 3 - batch(i): 780 , loss: 0.8157873749732971\n",">>> time: [09:28:08] training ep: 3 - batch(i): 790 , loss: 0.5675231218338013\n",">>> time: [09:28:09] training ep: 3 - batch(i): 800 , loss: 0.4784828722476959\n",">>> time: [09:28:10] training ep: 3 - batch(i): 810 , loss: 0.47157931327819824\n",">>> time: [09:28:11] training ep: 3 - batch(i): 820 , loss: 1.0973632335662842\n",">>> time: [09:28:12] training ep: 3 - batch(i): 830 , loss: 0.6467233300209045\n",">>> time: [09:28:13] training ep: 3 - batch(i): 840 , loss: 0.8120266795158386\n",">>> time: [09:28:14] training ep: 3 - batch(i): 850 , loss: 0.48111385107040405\n",">>> time: [09:28:14] training ep: 3 - batch(i): 860 , loss: 0.6135687828063965\n",">>> time: [09:28:15] training ep: 3 - batch(i): 870 , loss: 0.3554496169090271\n",">>> time: [09:28:16] training ep: 3 - batch(i): 880 , loss: 1.4101917743682861\n",">>> time: [09:28:17] training ep: 3 - batch(i): 890 , loss: 0.8528152108192444\n",">>> time: [09:28:18] training ep: 3 - batch(i): 900 , loss: 1.0645397901535034\n",">>> time: [09:28:19] training ep: 3 - batch(i): 910 , loss: 0.8416553139686584\n",">>> time: [09:28:20] training ep: 3 - batch(i): 920 , loss: 0.979912281036377\n",">>> time: [09:28:21] training ep: 3 - batch(i): 930 , loss: 0.6642696857452393\n",">>> time: [09:28:22] training ep: 3 - batch(i): 940 , loss: 0.6940240263938904\n",">>> time: [09:28:23] training ep: 3 - batch(i): 950 , loss: 0.648224949836731\n",">>> time: [09:28:24] training ep: 3 - batch(i): 960 , loss: 0.7008739709854126\n",">>> time: [09:28:24] training ep: 3 - batch(i): 970 , loss: 0.9162307977676392\n",">>> time: [09:28:25] training ep: 3 - batch(i): 980 , loss: 0.6245657801628113\n",">>> time: [09:28:26] training ep: 3 - batch(i): 990 , loss: 0.6308098435401917\n",">>> time: [09:28:27] training ep: 3 - batch(i): 1000 , loss: 0.768305242061615\n",">>> time: [09:28:28] training ep: 3 - batch(i): 1010 , loss: 0.6600627899169922\n",">>> time: [09:28:29] training ep: 3 - batch(i): 1020 , loss: 0.5278990864753723\n",">>> time: [09:28:30] training ep: 3 - batch(i): 1030 , loss: 1.1321892738342285\n",">>> time: [09:28:31] training ep: 3 - batch(i): 1040 , loss: 0.6348873972892761\n",">>> time: [09:28:32] training ep: 3 - batch(i): 1050 , loss: 0.3965122699737549\n",">>> time: [09:28:33] training ep: 3 - batch(i): 1060 , loss: 0.8729075789451599\n",">>> time: [09:28:34] training ep: 3 - batch(i): 1070 , loss: 0.5994335412979126\n",">>> time: [09:28:35] training ep: 3 - batch(i): 1080 , loss: 0.5165011286735535\n",">>> time: [09:28:35] training ep: 3 - batch(i): 1090 , loss: 1.3512334823608398\n",">>> time: [09:28:36] training ep: 3 - batch(i): 1100 , loss: 0.851231575012207\n",">>> time: [09:28:37] training ep: 3 - batch(i): 1110 , loss: 0.6457133889198303\n",">>> time: [09:28:38] training ep: 3 - batch(i): 1120 , loss: 0.5190604329109192\n",">>> time: [09:28:39] training ep: 3 - batch(i): 1130 , loss: 0.4201275408267975\n",">>> time: [09:28:40] training ep: 3 - batch(i): 1140 , loss: 0.8425742387771606\n",">>> time: [09:28:41] training ep: 3 - batch(i): 1150 , loss: 0.5045472979545593\n",">>> time: [09:28:42] training ep: 3 - batch(i): 1160 , loss: 0.48429787158966064\n",">>> time: [09:28:43] training ep: 3 - batch(i): 1170 , loss: 0.7200935482978821\n",">>> time: [09:28:44] training ep: 3 - batch(i): 1180 , loss: 0.6273576021194458\n",">>> time: [09:28:45] training ep: 3 - batch(i): 1190 , loss: 0.37138083577156067\n",">>> time: [09:28:46] training ep: 3 - batch(i): 1200 , loss: 0.43034273386001587\n",">>> time: [09:28:46] training ep: 3 - batch(i): 1210 , loss: 0.6968235969543457\n",">>> time: [09:28:47] training ep: 3 - batch(i): 1220 , loss: 0.5231266021728516\n",">>> time: [09:28:48] training ep: 3 - batch(i): 1230 , loss: 0.3523810803890228\n",">>> time: [09:28:49] training ep: 3 - batch(i): 1240 , loss: 0.6381081342697144\n",">>> time: [09:28:50] training ep: 3 - batch(i): 1250 , loss: 0.9072593450546265\n",">>> time: [09:28:51] training ep: 3 - batch(i): 1260 , loss: 1.0605531930923462\n",">>> time: [09:28:52] training ep: 3 - batch(i): 1270 , loss: 1.7359331846237183\n",">>> time: [09:28:53] training ep: 3 - batch(i): 1280 , loss: 0.3801764249801636\n",">>> time: [09:28:54] training ep: 3 - batch(i): 1290 , loss: 0.7589378356933594\n",">>> time: [09:28:55] training ep: 3 - batch(i): 1300 , loss: 0.3288508653640747\n",">>> time: [09:28:55] training ep: 3 - batch(i): 1310 , loss: 0.6599524021148682\n",">>> time: [09:28:56] training ep: 3 - batch(i): 1320 , loss: 0.670379638671875\n",">>> time: [09:28:57] training ep: 3 - batch(i): 1330 , loss: 0.57505863904953\n",">>> time: [09:28:58] training ep: 3 - batch(i): 1340 , loss: 0.7001627087593079\n",">>> time: [09:28:59] training ep: 3 - batch(i): 1350 , loss: 0.41188451647758484\n",">>> time: [09:29:00] training ep: 3 - batch(i): 1360 , loss: 0.2819790840148926\n",">>> time: [09:29:01] training ep: 3 - batch(i): 1370 , loss: 0.40054506063461304\n",">>> time: [09:29:02] training ep: 3 - batch(i): 1380 , loss: 0.9450592398643494\n",">>> time: [09:29:03] training ep: 3 - batch(i): 1390 , loss: 0.7093127965927124\n",">>> time: [09:29:04] training ep: 3 - batch(i): 1400 , loss: 1.2761071920394897\n",">>> time: [09:29:05] training ep: 3 - batch(i): 1410 , loss: 0.5210464596748352\n",">>> time: [09:29:05] training ep: 3 - batch(i): 1420 , loss: 1.3367539644241333\n",">>> time: [09:29:06] training ep: 3 - batch(i): 1430 , loss: 0.6145338416099548\n",">>> time: [09:29:07] training ep: 3 - batch(i): 1440 , loss: 0.4135846495628357\n",">>> time: [09:29:08] training ep: 3 - batch(i): 1450 , loss: 0.3052820861339569\n",">>> time: [09:29:09] training ep: 3 - batch(i): 1460 , loss: 0.5480700135231018\n",">>> time: [09:29:10] training ep: 3 - batch(i): 1470 , loss: 0.3313966393470764\n",">>> time: [09:29:11] training ep: 3 - batch(i): 1480 , loss: 0.8886658549308777\n",">>> time: [09:29:12] training ep: 3 - batch(i): 1490 , loss: 0.2540755271911621\n",">>> time: [09:29:13] training ep: 3 - batch(i): 1500 , loss: 0.4385421574115753\n",">>> time: [09:29:14] training ep: 3 - batch(i): 1510 , loss: 0.43538275361061096\n",">>> time: [09:29:15] training ep: 3 - batch(i): 1520 , loss: 0.4826984703540802\n",">>> time: [09:29:16] training ep: 3 - batch(i): 1530 , loss: 0.5418283939361572\n",">>> time: [09:29:16] training ep: 3 - batch(i): 1540 , loss: 0.7236833572387695\n",">>> time: [09:29:17] training ep: 3 - batch(i): 1550 , loss: 0.4456757605075836\n",">>> time: [09:29:18] training ep: 3 - batch(i): 1560 , loss: 0.3859126567840576\n",">>> time: [09:29:19] training ep: 3 - batch(i): 1570 , loss: 0.2490636110305786\n",">>> time: [09:29:20] training ep: 3 - batch(i): 1580 , loss: 0.5218121409416199\n",">>> time: [09:29:21] training ep: 3 - batch(i): 1590 , loss: 0.6940615177154541\n",">>> time: [09:29:22] training ep: 3 - batch(i): 1600 , loss: 0.6099916696548462\n",">>> time: [09:29:23] training ep: 3 - batch(i): 1610 , loss: 0.8467205166816711\n",">>> time: [09:29:24] training ep: 3 - batch(i): 1620 , loss: 0.4353344440460205\n",">>> time: [09:29:25] training ep: 3 - batch(i): 1630 , loss: 0.8924465179443359\n",">>> time: [09:29:25] training ep: 3 - batch(i): 1640 , loss: 0.29387909173965454\n",">>> time: [09:29:26] training ep: 3 - batch(i): 1650 , loss: 0.9252193570137024\n",">>> time: [09:29:27] training ep: 3 - batch(i): 1660 , loss: 0.5391554236412048\n",">>> time: [09:29:28] training ep: 3 - batch(i): 1670 , loss: 0.49550172686576843\n",">>> time: [09:29:29] training ep: 3 - batch(i): 1680 , loss: 0.5436357259750366\n",">>> time: [09:29:30] training ep: 3 - batch(i): 1690 , loss: 1.2282954454421997\n",">>> time: [09:29:31] training ep: 3 - batch(i): 1700 , loss: 0.3390345275402069\n",">>> time: [09:29:32] training ep: 3 - batch(i): 1710 , loss: 1.2563589811325073\n",">>> time: [09:29:33] training ep: 3 - batch(i): 1720 , loss: 1.082651138305664\n",">>> time: [09:29:34] training ep: 3 - batch(i): 1730 , loss: 0.8868328928947449\n",">>> time: [09:29:35] training ep: 3 - batch(i): 1740 , loss: 0.5970115065574646\n",">>> time: [09:29:36] training ep: 3 - batch(i): 1750 , loss: 0.4369230270385742\n",">>> time: [09:29:36] training ep: 3 - batch(i): 1760 , loss: 0.37793925404548645\n",">>> time: [09:29:37] training ep: 3 - batch(i): 1770 , loss: 0.28613969683647156\n",">>> time: [09:29:38] training ep: 3 - batch(i): 1780 , loss: 0.41455674171447754\n",">>> time: [09:29:39] training ep: 3 - batch(i): 1790 , loss: 0.4441928267478943\n",">>> time: [09:29:40] training ep: 3 - batch(i): 1800 , loss: 0.37966039776802063\n",">>> time: [09:29:41] training ep: 3 - batch(i): 1810 , loss: 0.336404412984848\n",">>> time: [09:29:42] training ep: 3 - batch(i): 1820 , loss: 0.3154151737689972\n",">>> time: [09:29:43] training ep: 3 - batch(i): 1830 , loss: 0.5328081846237183\n",">>> time: [09:29:44] training ep: 3 - batch(i): 1840 , loss: 0.4755784869194031\n",">>> time: [09:29:45] training ep: 3 - batch(i): 1850 , loss: 0.42990100383758545\n",">>> time: [09:29:46] training ep: 3 - batch(i): 1860 , loss: 0.35870081186294556\n",">>> time: [09:29:46] training ep: 3 - batch(i): 1870 , loss: 0.47074562311172485\n",">>> time: [09:29:47] training ep: 3 - batch(i): 1880 , loss: 0.45841482281684875\n",">>> time: [09:29:48] training ep: 3 - batch(i): 1890 , loss: 0.3674021363258362\n",">>> time: [09:29:49] training ep: 3 - batch(i): 1900 , loss: 0.44913598895072937\n",">>> time: [09:29:50] training ep: 3 - batch(i): 1910 , loss: 0.4205978214740753\n",">>> time: [09:29:51] training ep: 3 - batch(i): 1920 , loss: 0.3050263524055481\n",">>> time: [09:29:52] training ep: 3 - batch(i): 1930 , loss: 0.36286258697509766\n",">>> time: [09:29:53] training ep: 3 - batch(i): 1940 , loss: 0.6496620178222656\n",">>> time: [09:29:54] training ep: 3 - batch(i): 1950 , loss: 0.5012556910514832\n",">>> time: [09:29:55] training ep: 3 - batch(i): 1960 , loss: 0.6173905730247498\n",">>> time: [09:29:56] training ep: 3 - batch(i): 1970 , loss: 0.45582133531570435\n",">>> time: [09:29:56] training ep: 3 - batch(i): 1980 , loss: 0.4585258364677429\n",">>> time: [09:29:57] training ep: 3 - batch(i): 1990 , loss: 0.41112834215164185\n",">>> time: [09:29:58] training ep: 3 - batch(i): 2000 , loss: 0.3117975890636444\n",">>> time: [09:29:59] training ep: 3 - batch(i): 2010 , loss: 0.3138872981071472\n",">>> time: [09:30:00] training ep: 3 - batch(i): 2020 , loss: 0.44985201954841614\n",">>> time: [09:30:01] training ep: 3 - batch(i): 2030 , loss: 0.7176699042320251\n",">>> time: [09:30:02] training ep: 3 - batch(i): 2040 , loss: 0.4256923496723175\n",">>> time: [09:30:03] training ep: 3 - batch(i): 2050 , loss: 0.5374267101287842\n",">>> time: [09:30:04] training ep: 3 - batch(i): 2060 , loss: 0.2569679915904999\n",">>> time: [09:30:05] training ep: 3 - batch(i): 2070 , loss: 0.617931604385376\n",">>> time: [09:30:05] training ep: 3 - batch(i): 2080 , loss: 0.30585190653800964\n",">>> time: [09:30:06] training ep: 3 - batch(i): 2090 , loss: 0.3414488434791565\n",">>> time: [09:30:07] training ep: 3 - batch(i): 2100 , loss: 0.6142578721046448\n",">>> time: [09:30:08] training ep: 3 - batch(i): 2110 , loss: 0.4209566116333008\n",">>> time: [09:30:09] training ep: 3 - batch(i): 2120 , loss: 0.254405677318573\n",">>> time: [09:30:10] training ep: 3 - batch(i): 2130 , loss: 0.443236380815506\n",">>> time: [09:30:11] training ep: 3 - batch(i): 2140 , loss: 0.3457583487033844\n",">>> time: [09:30:12] training ep: 3 - batch(i): 2150 , loss: 0.3040708899497986\n",">>> time: [09:30:13] training ep: 3 - batch(i): 2160 , loss: 0.4866180419921875\n",">>> time: [09:30:14] training ep: 3 - batch(i): 2170 , loss: 0.44469335675239563\n",">>> time: [09:30:14] training ep: 3 - batch(i): 2180 , loss: 0.5071417689323425\n",">>> time: [09:30:15] training ep: 3 - batch(i): 2190 , loss: 0.6461334824562073\n",">>> time: [09:30:16] training ep: 3 - batch(i): 2200 , loss: 0.3231208920478821\n",">>> time: [09:30:17] training ep: 3 - batch(i): 2210 , loss: 0.636883020401001\n",">>> time: [09:30:18] training ep: 3 - batch(i): 2220 , loss: 0.3985632061958313\n",">>> time: [09:30:19] training ep: 3 - batch(i): 2230 , loss: 0.934063732624054\n",">>> time: [09:30:20] training ep: 3 - batch(i): 2240 , loss: 0.37693890929222107\n","training ep 4 - trainlen: 2242\n",">>> time: [09:30:22] training ep: 4 - batch(i): 0 , loss: 0.23234929144382477\n",">>> time: [09:30:23] training ep: 4 - batch(i): 10 , loss: 0.5837721824645996\n",">>> time: [09:30:24] training ep: 4 - batch(i): 20 , loss: 0.46294310688972473\n",">>> time: [09:30:25] training ep: 4 - batch(i): 30 , loss: 0.5254579186439514\n",">>> time: [09:30:25] training ep: 4 - batch(i): 40 , loss: 0.40856561064720154\n",">>> time: [09:30:26] training ep: 4 - batch(i): 50 , loss: 0.5240041613578796\n",">>> time: [09:30:27] training ep: 4 - batch(i): 60 , loss: 0.6903060078620911\n",">>> time: [09:30:28] training ep: 4 - batch(i): 70 , loss: 1.0545254945755005\n",">>> time: [09:30:29] training ep: 4 - batch(i): 80 , loss: 0.3956116735935211\n",">>> time: [09:30:30] training ep: 4 - batch(i): 90 , loss: 0.4358629584312439\n",">>> time: [09:30:31] training ep: 4 - batch(i): 100 , loss: 0.22732609510421753\n",">>> time: [09:30:32] training ep: 4 - batch(i): 110 , loss: 0.5018131732940674\n",">>> time: [09:30:33] training ep: 4 - batch(i): 120 , loss: 0.5863422155380249\n",">>> time: [09:30:34] training ep: 4 - batch(i): 130 , loss: 0.728049099445343\n",">>> time: [09:30:35] training ep: 4 - batch(i): 140 , loss: 0.5897639989852905\n",">>> time: [09:30:36] training ep: 4 - batch(i): 150 , loss: 0.44123202562332153\n",">>> time: [09:30:37] training ep: 4 - batch(i): 160 , loss: 0.5366338491439819\n",">>> time: [09:30:38] training ep: 4 - batch(i): 170 , loss: 0.4199276268482208\n",">>> time: [09:30:39] training ep: 4 - batch(i): 180 , loss: 0.46339908242225647\n",">>> time: [09:30:40] training ep: 4 - batch(i): 190 , loss: 0.8004814982414246\n",">>> time: [09:30:40] training ep: 4 - batch(i): 200 , loss: 0.5359412431716919\n",">>> time: [09:30:41] training ep: 4 - batch(i): 210 , loss: 1.1374027729034424\n",">>> time: [09:30:42] training ep: 4 - batch(i): 220 , loss: 1.3274269104003906\n",">>> time: [09:30:43] training ep: 4 - batch(i): 230 , loss: 0.6885613799095154\n",">>> time: [09:30:44] training ep: 4 - batch(i): 240 , loss: 0.8248199820518494\n",">>> time: [09:30:45] training ep: 4 - batch(i): 250 , loss: 0.5122614502906799\n",">>> time: [09:30:46] training ep: 4 - batch(i): 260 , loss: 1.1033046245574951\n",">>> time: [09:30:47] training ep: 4 - batch(i): 270 , loss: 0.4375581443309784\n",">>> time: [09:30:48] training ep: 4 - batch(i): 280 , loss: 0.8192402124404907\n",">>> time: [09:30:49] training ep: 4 - batch(i): 290 , loss: 0.7727415561676025\n",">>> time: [09:30:50] training ep: 4 - batch(i): 300 , loss: 0.5813083648681641\n",">>> time: [09:30:50] training ep: 4 - batch(i): 310 , loss: 0.5188146829605103\n",">>> time: [09:30:51] training ep: 4 - batch(i): 320 , loss: 0.31885451078414917\n",">>> time: [09:30:52] training ep: 4 - batch(i): 330 , loss: 0.42033836245536804\n",">>> time: [09:30:53] training ep: 4 - batch(i): 340 , loss: 0.6135470867156982\n",">>> time: [09:30:54] training ep: 4 - batch(i): 350 , loss: 0.43382808566093445\n",">>> time: [09:30:55] training ep: 4 - batch(i): 360 , loss: 1.119519591331482\n",">>> time: [09:30:56] training ep: 4 - batch(i): 370 , loss: 0.45190954208374023\n",">>> time: [09:30:57] training ep: 4 - batch(i): 380 , loss: 0.4301159381866455\n",">>> time: [09:30:58] training ep: 4 - batch(i): 390 , loss: 0.34690263867378235\n",">>> time: [09:30:59] training ep: 4 - batch(i): 400 , loss: 0.5179428458213806\n",">>> time: [09:31:00] training ep: 4 - batch(i): 410 , loss: 1.1635980606079102\n",">>> time: [09:31:00] training ep: 4 - batch(i): 420 , loss: 0.4025840759277344\n",">>> time: [09:31:01] training ep: 4 - batch(i): 430 , loss: 0.5608481168746948\n",">>> time: [09:31:02] training ep: 4 - batch(i): 440 , loss: 0.29055291414260864\n",">>> time: [09:31:03] training ep: 4 - batch(i): 450 , loss: 0.6928938031196594\n",">>> time: [09:31:04] training ep: 4 - batch(i): 460 , loss: 0.58610600233078\n",">>> time: [09:31:05] training ep: 4 - batch(i): 470 , loss: 0.9118791222572327\n",">>> time: [09:31:06] training ep: 4 - batch(i): 480 , loss: 0.3810551166534424\n",">>> time: [09:31:07] training ep: 4 - batch(i): 490 , loss: 0.4241723120212555\n",">>> time: [09:31:08] training ep: 4 - batch(i): 500 , loss: 0.4156090319156647\n",">>> time: [09:31:09] training ep: 4 - batch(i): 510 , loss: 0.5223647356033325\n",">>> time: [09:31:10] training ep: 4 - batch(i): 520 , loss: 0.8372592329978943\n",">>> time: [09:31:10] training ep: 4 - batch(i): 530 , loss: 0.44236430525779724\n",">>> time: [09:31:11] training ep: 4 - batch(i): 540 , loss: 0.58568274974823\n",">>> time: [09:31:12] training ep: 4 - batch(i): 550 , loss: 0.43253403902053833\n",">>> time: [09:31:13] training ep: 4 - batch(i): 560 , loss: 1.264571189880371\n",">>> time: [09:31:14] training ep: 4 - batch(i): 570 , loss: 0.6276309490203857\n",">>> time: [09:31:15] training ep: 4 - batch(i): 580 , loss: 0.4134574830532074\n",">>> time: [09:31:16] training ep: 4 - batch(i): 590 , loss: 0.2816126346588135\n",">>> time: [09:31:17] training ep: 4 - batch(i): 600 , loss: 0.3165493309497833\n",">>> time: [09:31:18] training ep: 4 - batch(i): 610 , loss: 0.8097662329673767\n",">>> time: [09:31:19] training ep: 4 - batch(i): 620 , loss: 0.5327346920967102\n",">>> time: [09:31:19] training ep: 4 - batch(i): 630 , loss: 0.3886731266975403\n",">>> time: [09:31:20] training ep: 4 - batch(i): 640 , loss: 0.46233659982681274\n",">>> time: [09:31:21] training ep: 4 - batch(i): 650 , loss: 0.35565271973609924\n",">>> time: [09:31:22] training ep: 4 - batch(i): 660 , loss: 0.7892112135887146\n",">>> time: [09:31:23] training ep: 4 - batch(i): 670 , loss: 0.6743528842926025\n",">>> time: [09:31:24] training ep: 4 - batch(i): 680 , loss: 0.3237796127796173\n",">>> time: [09:31:25] training ep: 4 - batch(i): 690 , loss: 0.6357917785644531\n",">>> time: [09:31:26] training ep: 4 - batch(i): 700 , loss: 0.37675121426582336\n",">>> time: [09:31:27] training ep: 4 - batch(i): 710 , loss: 0.5908185243606567\n",">>> time: [09:31:28] training ep: 4 - batch(i): 720 , loss: 0.5202067494392395\n",">>> time: [09:31:29] training ep: 4 - batch(i): 730 , loss: 0.6693912744522095\n",">>> time: [09:31:29] training ep: 4 - batch(i): 740 , loss: 0.44479605555534363\n",">>> time: [09:31:30] training ep: 4 - batch(i): 750 , loss: 0.3782404363155365\n",">>> time: [09:31:31] training ep: 4 - batch(i): 760 , loss: 0.539344310760498\n",">>> time: [09:31:32] training ep: 4 - batch(i): 770 , loss: 0.5858820676803589\n",">>> time: [09:31:33] training ep: 4 - batch(i): 780 , loss: 0.8060913681983948\n",">>> time: [09:31:34] training ep: 4 - batch(i): 790 , loss: 0.8133713603019714\n",">>> time: [09:31:35] training ep: 4 - batch(i): 800 , loss: 0.3558097183704376\n",">>> time: [09:31:36] training ep: 4 - batch(i): 810 , loss: 0.6837796568870544\n",">>> time: [09:31:37] training ep: 4 - batch(i): 820 , loss: 0.6078324317932129\n",">>> time: [09:31:37] training ep: 4 - batch(i): 830 , loss: 0.7193708419799805\n",">>> time: [09:31:38] training ep: 4 - batch(i): 840 , loss: 0.5924216508865356\n",">>> time: [09:31:39] training ep: 4 - batch(i): 850 , loss: 0.6605847477912903\n",">>> time: [09:31:40] training ep: 4 - batch(i): 860 , loss: 0.3922386169433594\n",">>> time: [09:31:41] training ep: 4 - batch(i): 870 , loss: 0.407956600189209\n",">>> time: [09:31:42] training ep: 4 - batch(i): 880 , loss: 0.4399052858352661\n",">>> time: [09:31:43] training ep: 4 - batch(i): 890 , loss: 0.6512030363082886\n",">>> time: [09:31:44] training ep: 4 - batch(i): 900 , loss: 0.396786630153656\n",">>> time: [09:31:44] training ep: 4 - batch(i): 910 , loss: 0.4205453395843506\n",">>> time: [09:31:45] training ep: 4 - batch(i): 920 , loss: 0.766385555267334\n",">>> time: [09:31:46] training ep: 4 - batch(i): 930 , loss: 0.7351190447807312\n",">>> time: [09:31:47] training ep: 4 - batch(i): 940 , loss: 0.619066059589386\n",">>> time: [09:31:48] training ep: 4 - batch(i): 950 , loss: 0.9725127816200256\n",">>> time: [09:31:49] training ep: 4 - batch(i): 960 , loss: 1.3183119297027588\n",">>> time: [09:31:50] training ep: 4 - batch(i): 970 , loss: 0.40676653385162354\n",">>> time: [09:31:51] training ep: 4 - batch(i): 980 , loss: 0.31548255681991577\n",">>> time: [09:31:52] training ep: 4 - batch(i): 990 , loss: 0.3668292164802551\n",">>> time: [09:31:53] training ep: 4 - batch(i): 1000 , loss: 0.402972936630249\n",">>> time: [09:31:53] training ep: 4 - batch(i): 1010 , loss: 0.6834307312965393\n",">>> time: [09:31:54] training ep: 4 - batch(i): 1020 , loss: 0.2755994498729706\n",">>> time: [09:31:55] training ep: 4 - batch(i): 1030 , loss: 0.7895715832710266\n",">>> time: [09:31:56] training ep: 4 - batch(i): 1040 , loss: 0.347550630569458\n",">>> time: [09:31:57] training ep: 4 - batch(i): 1050 , loss: 0.4222831428050995\n",">>> time: [09:31:58] training ep: 4 - batch(i): 1060 , loss: 0.4622349143028259\n",">>> time: [09:31:59] training ep: 4 - batch(i): 1070 , loss: 0.3133356273174286\n",">>> time: [09:32:00] training ep: 4 - batch(i): 1080 , loss: 0.2755638360977173\n",">>> time: [09:32:01] training ep: 4 - batch(i): 1090 , loss: 0.7373791337013245\n",">>> time: [09:32:01] training ep: 4 - batch(i): 1100 , loss: 0.4497050344944\n",">>> time: [09:32:02] training ep: 4 - batch(i): 1110 , loss: 0.3860892653465271\n",">>> time: [09:32:03] training ep: 4 - batch(i): 1120 , loss: 0.47706353664398193\n",">>> time: [09:32:04] training ep: 4 - batch(i): 1130 , loss: 0.3872089385986328\n",">>> time: [09:32:05] training ep: 4 - batch(i): 1140 , loss: 0.20066623389720917\n",">>> time: [09:32:06] training ep: 4 - batch(i): 1150 , loss: 0.6217540502548218\n",">>> time: [09:32:07] training ep: 4 - batch(i): 1160 , loss: 0.2787078619003296\n",">>> time: [09:32:08] training ep: 4 - batch(i): 1170 , loss: 0.3798675537109375\n",">>> time: [09:32:09] training ep: 4 - batch(i): 1180 , loss: 0.4103628098964691\n",">>> time: [09:32:10] training ep: 4 - batch(i): 1190 , loss: 0.32959458231925964\n",">>> time: [09:32:10] training ep: 4 - batch(i): 1200 , loss: 0.8282483220100403\n",">>> time: [09:32:11] training ep: 4 - batch(i): 1210 , loss: 0.366634339094162\n",">>> time: [09:32:12] training ep: 4 - batch(i): 1220 , loss: 0.2232283502817154\n",">>> time: [09:32:13] training ep: 4 - batch(i): 1230 , loss: 0.3274540603160858\n",">>> time: [09:32:14] training ep: 4 - batch(i): 1240 , loss: 0.8228165507316589\n",">>> time: [09:32:15] training ep: 4 - batch(i): 1250 , loss: 1.1674553155899048\n",">>> time: [09:32:16] training ep: 4 - batch(i): 1260 , loss: 0.5152218341827393\n",">>> time: [09:32:17] training ep: 4 - batch(i): 1270 , loss: 0.3397063910961151\n",">>> time: [09:32:18] training ep: 4 - batch(i): 1280 , loss: 0.3560682237148285\n",">>> time: [09:32:19] training ep: 4 - batch(i): 1290 , loss: 0.24001668393611908\n",">>> time: [09:32:20] training ep: 4 - batch(i): 1300 , loss: 0.2954934537410736\n",">>> time: [09:32:20] training ep: 4 - batch(i): 1310 , loss: 0.2500418722629547\n",">>> time: [09:32:21] training ep: 4 - batch(i): 1320 , loss: 0.47908857464790344\n",">>> time: [09:32:22] training ep: 4 - batch(i): 1330 , loss: 0.45251160860061646\n",">>> time: [09:32:23] training ep: 4 - batch(i): 1340 , loss: 0.4543340802192688\n",">>> time: [09:32:24] training ep: 4 - batch(i): 1350 , loss: 0.7478938102722168\n",">>> time: [09:32:25] training ep: 4 - batch(i): 1360 , loss: 0.3270599842071533\n",">>> time: [09:32:26] training ep: 4 - batch(i): 1370 , loss: 0.406936913728714\n",">>> time: [09:32:27] training ep: 4 - batch(i): 1380 , loss: 0.2791490852832794\n",">>> time: [09:32:28] training ep: 4 - batch(i): 1390 , loss: 0.3493483364582062\n",">>> time: [09:32:29] training ep: 4 - batch(i): 1400 , loss: 0.2940428555011749\n",">>> time: [09:32:29] training ep: 4 - batch(i): 1410 , loss: 0.5762413740158081\n",">>> time: [09:32:30] training ep: 4 - batch(i): 1420 , loss: 0.3297938406467438\n",">>> time: [09:32:31] training ep: 4 - batch(i): 1430 , loss: 0.4635298252105713\n",">>> time: [09:32:32] training ep: 4 - batch(i): 1440 , loss: 0.42413705587387085\n",">>> time: [09:32:33] training ep: 4 - batch(i): 1450 , loss: 0.5812665820121765\n",">>> time: [09:32:34] training ep: 4 - batch(i): 1460 , loss: 0.48049473762512207\n",">>> time: [09:32:35] training ep: 4 - batch(i): 1470 , loss: 0.3859698474407196\n",">>> time: [09:32:36] training ep: 4 - batch(i): 1480 , loss: 0.35922279953956604\n",">>> time: [09:32:37] training ep: 4 - batch(i): 1490 , loss: 0.40914419293403625\n",">>> time: [09:32:38] training ep: 4 - batch(i): 1500 , loss: 0.33606964349746704\n",">>> time: [09:32:39] training ep: 4 - batch(i): 1510 , loss: 0.5119432806968689\n",">>> time: [09:32:39] training ep: 4 - batch(i): 1520 , loss: 0.36823770403862\n",">>> time: [09:32:40] training ep: 4 - batch(i): 1530 , loss: 0.29214540123939514\n",">>> time: [09:32:41] training ep: 4 - batch(i): 1540 , loss: 0.3437437415122986\n",">>> time: [09:32:42] training ep: 4 - batch(i): 1550 , loss: 1.0091347694396973\n",">>> time: [09:32:43] training ep: 4 - batch(i): 1560 , loss: 0.4138060212135315\n",">>> time: [09:32:44] training ep: 4 - batch(i): 1570 , loss: 0.2612319886684418\n",">>> time: [09:32:45] training ep: 4 - batch(i): 1580 , loss: 0.32133400440216064\n",">>> time: [09:32:46] training ep: 4 - batch(i): 1590 , loss: 0.47491252422332764\n",">>> time: [09:32:47] training ep: 4 - batch(i): 1600 , loss: 0.23413681983947754\n",">>> time: [09:32:47] training ep: 4 - batch(i): 1610 , loss: 0.29887890815734863\n",">>> time: [09:32:48] training ep: 4 - batch(i): 1620 , loss: 0.42214247584342957\n",">>> time: [09:32:49] training ep: 4 - batch(i): 1630 , loss: 0.3489505350589752\n",">>> time: [09:32:50] training ep: 4 - batch(i): 1640 , loss: 0.25002941489219666\n",">>> time: [09:32:51] training ep: 4 - batch(i): 1650 , loss: 0.24673059582710266\n",">>> time: [09:32:52] training ep: 4 - batch(i): 1660 , loss: 0.29762402176856995\n",">>> time: [09:32:53] training ep: 4 - batch(i): 1670 , loss: 0.6980958580970764\n",">>> time: [09:32:54] training ep: 4 - batch(i): 1680 , loss: 0.4956696331501007\n",">>> time: [09:32:55] training ep: 4 - batch(i): 1690 , loss: 0.2590932250022888\n",">>> time: [09:32:56] training ep: 4 - batch(i): 1700 , loss: 0.38849931955337524\n",">>> time: [09:32:57] training ep: 4 - batch(i): 1710 , loss: 0.3159106969833374\n",">>> time: [09:32:57] training ep: 4 - batch(i): 1720 , loss: 0.31947654485702515\n",">>> time: [09:32:58] training ep: 4 - batch(i): 1730 , loss: 0.18609239161014557\n",">>> time: [09:32:59] training ep: 4 - batch(i): 1740 , loss: 0.9132646918296814\n",">>> time: [09:33:00] training ep: 4 - batch(i): 1750 , loss: 0.8844989538192749\n",">>> time: [09:33:01] training ep: 4 - batch(i): 1760 , loss: 0.28323349356651306\n",">>> time: [09:33:02] training ep: 4 - batch(i): 1770 , loss: 0.16164249181747437\n",">>> time: [09:33:03] training ep: 4 - batch(i): 1780 , loss: 0.258249431848526\n",">>> time: [09:33:04] training ep: 4 - batch(i): 1790 , loss: 0.21837876737117767\n",">>> time: [09:33:05] training ep: 4 - batch(i): 1800 , loss: 0.26977020502090454\n",">>> time: [09:33:06] training ep: 4 - batch(i): 1810 , loss: 0.36987394094467163\n",">>> time: [09:33:06] training ep: 4 - batch(i): 1820 , loss: 0.4932042062282562\n",">>> time: [09:33:07] training ep: 4 - batch(i): 1830 , loss: 0.25067803263664246\n",">>> time: [09:33:08] training ep: 4 - batch(i): 1840 , loss: 0.3027864992618561\n",">>> time: [09:33:09] training ep: 4 - batch(i): 1850 , loss: 0.5890491008758545\n",">>> time: [09:33:10] training ep: 4 - batch(i): 1860 , loss: 0.3100980520248413\n",">>> time: [09:33:11] training ep: 4 - batch(i): 1870 , loss: 0.33083194494247437\n",">>> time: [09:33:12] training ep: 4 - batch(i): 1880 , loss: 0.36844927072525024\n",">>> time: [09:33:13] training ep: 4 - batch(i): 1890 , loss: 0.27578049898147583\n",">>> time: [09:33:14] training ep: 4 - batch(i): 1900 , loss: 0.23566590249538422\n",">>> time: [09:33:15] training ep: 4 - batch(i): 1910 , loss: 0.17776069045066833\n",">>> time: [09:33:16] training ep: 4 - batch(i): 1920 , loss: 0.27502724528312683\n",">>> time: [09:33:16] training ep: 4 - batch(i): 1930 , loss: 0.612259030342102\n",">>> time: [09:33:17] training ep: 4 - batch(i): 1940 , loss: 0.4631648361682892\n",">>> time: [09:33:18] training ep: 4 - batch(i): 1950 , loss: 0.22127725183963776\n",">>> time: [09:33:19] training ep: 4 - batch(i): 1960 , loss: 0.33909872174263\n",">>> time: [09:33:20] training ep: 4 - batch(i): 1970 , loss: 0.3221016526222229\n",">>> time: [09:33:21] training ep: 4 - batch(i): 1980 , loss: 0.3517360985279083\n",">>> time: [09:33:22] training ep: 4 - batch(i): 1990 , loss: 0.3260185122489929\n",">>> time: [09:33:23] training ep: 4 - batch(i): 2000 , loss: 0.267320454120636\n",">>> time: [09:33:24] training ep: 4 - batch(i): 2010 , loss: 0.2902947664260864\n",">>> time: [09:33:25] training ep: 4 - batch(i): 2020 , loss: 0.3320717513561249\n",">>> time: [09:33:26] training ep: 4 - batch(i): 2030 , loss: 0.4421915113925934\n",">>> time: [09:33:26] training ep: 4 - batch(i): 2040 , loss: 0.5124427080154419\n",">>> time: [09:33:27] training ep: 4 - batch(i): 2050 , loss: 0.11435560882091522\n",">>> time: [09:33:28] training ep: 4 - batch(i): 2060 , loss: 0.22202900052070618\n",">>> time: [09:33:29] training ep: 4 - batch(i): 2070 , loss: 0.17098689079284668\n",">>> time: [09:33:30] training ep: 4 - batch(i): 2080 , loss: 0.3902236819267273\n",">>> time: [09:33:31] training ep: 4 - batch(i): 2090 , loss: 0.4190985858440399\n",">>> time: [09:33:32] training ep: 4 - batch(i): 2100 , loss: 0.3143206536769867\n",">>> time: [09:33:33] training ep: 4 - batch(i): 2110 , loss: 0.3806491792201996\n",">>> time: [09:33:34] training ep: 4 - batch(i): 2120 , loss: 0.21545445919036865\n",">>> time: [09:33:34] training ep: 4 - batch(i): 2130 , loss: 0.3043002188205719\n",">>> time: [09:33:35] training ep: 4 - batch(i): 2140 , loss: 0.5528106689453125\n",">>> time: [09:33:36] training ep: 4 - batch(i): 2150 , loss: 0.36202168464660645\n",">>> time: [09:33:37] training ep: 4 - batch(i): 2160 , loss: 0.2794078588485718\n",">>> time: [09:33:38] training ep: 4 - batch(i): 2170 , loss: 0.29653701186180115\n",">>> time: [09:33:39] training ep: 4 - batch(i): 2180 , loss: 0.2690858542919159\n",">>> time: [09:33:40] training ep: 4 - batch(i): 2190 , loss: 0.21858033537864685\n",">>> time: [09:33:41] training ep: 4 - batch(i): 2200 , loss: 0.30868545174598694\n",">>> time: [09:33:42] training ep: 4 - batch(i): 2210 , loss: 0.3364787995815277\n",">>> time: [09:33:43] training ep: 4 - batch(i): 2220 , loss: 0.22184808552265167\n",">>> time: [09:33:43] training ep: 4 - batch(i): 2230 , loss: 0.38977178931236267\n",">>> time: [09:33:44] training ep: 4 - batch(i): 2240 , loss: 0.5854983329772949\n","training complete, save results? [y/n] : y\n","command not recognised, enter y or n : y\n","saving weights to weights/...\n","weights and field pickles saved to weights\n","train for more epochs? [y/n] : 1\n","command not recognised, enter y or n : y\n","type number of epochs to train for : 1\n","training model...\n","training ep 0 - trainlen: 2242\n",">>> time: [09:36:57] training ep: 0 - batch(i): 0 , loss: 0.455453097820282\n",">>> time: [09:36:58] training ep: 0 - batch(i): 10 , loss: 0.2943633198738098\n",">>> time: [09:36:59] training ep: 0 - batch(i): 20 , loss: 0.22283171117305756\n",">>> time: [09:37:00] training ep: 0 - batch(i): 30 , loss: 0.3070986568927765\n",">>> time: [09:37:01] training ep: 0 - batch(i): 40 , loss: 0.19475509226322174\n",">>> time: [09:37:02] training ep: 0 - batch(i): 50 , loss: 0.2440534383058548\n",">>> time: [09:37:03] training ep: 0 - batch(i): 60 , loss: 0.33318209648132324\n",">>> time: [09:37:04] training ep: 0 - batch(i): 70 , loss: 0.2647154629230499\n",">>> time: [09:37:05] training ep: 0 - batch(i): 80 , loss: 0.6065013408660889\n",">>> time: [09:37:06] training ep: 0 - batch(i): 90 , loss: 0.562993586063385\n",">>> time: [09:37:06] training ep: 0 - batch(i): 100 , loss: 0.4124631881713867\n",">>> time: [09:37:07] training ep: 0 - batch(i): 110 , loss: 0.521245002746582\n",">>> time: [09:37:08] training ep: 0 - batch(i): 120 , loss: 0.6304066181182861\n",">>> time: [09:37:09] training ep: 0 - batch(i): 130 , loss: 0.3305848240852356\n",">>> time: [09:37:10] training ep: 0 - batch(i): 140 , loss: 0.754518985748291\n",">>> time: [09:37:11] training ep: 0 - batch(i): 150 , loss: 0.3098817467689514\n",">>> time: [09:37:12] training ep: 0 - batch(i): 160 , loss: 0.2135459929704666\n",">>> time: [09:37:13] training ep: 0 - batch(i): 170 , loss: 0.20580227673053741\n",">>> time: [09:37:14] training ep: 0 - batch(i): 180 , loss: 0.29011064767837524\n",">>> time: [09:37:15] training ep: 0 - batch(i): 190 , loss: 0.4436067044734955\n",">>> time: [09:37:16] training ep: 0 - batch(i): 200 , loss: 0.32607290148735046\n",">>> time: [09:37:17] training ep: 0 - batch(i): 210 , loss: 0.22197218239307404\n",">>> time: [09:37:18] training ep: 0 - batch(i): 220 , loss: 0.463497132062912\n",">>> time: [09:37:19] training ep: 0 - batch(i): 230 , loss: 0.22356510162353516\n",">>> time: [09:37:19] training ep: 0 - batch(i): 240 , loss: 0.44548365473747253\n",">>> time: [09:37:20] training ep: 0 - batch(i): 250 , loss: 0.3118862509727478\n",">>> time: [09:37:21] training ep: 0 - batch(i): 260 , loss: 0.32143890857696533\n",">>> time: [09:37:22] training ep: 0 - batch(i): 270 , loss: 0.439768522977829\n",">>> time: [09:37:23] training ep: 0 - batch(i): 280 , loss: 0.23837915062904358\n",">>> time: [09:37:24] training ep: 0 - batch(i): 290 , loss: 0.9036320447921753\n",">>> time: [09:37:25] training ep: 0 - batch(i): 300 , loss: 0.3321109116077423\n",">>> time: [09:37:26] training ep: 0 - batch(i): 310 , loss: 0.4367774426937103\n",">>> time: [09:37:27] training ep: 0 - batch(i): 320 , loss: 1.103334665298462\n",">>> time: [09:37:28] training ep: 0 - batch(i): 330 , loss: 0.9382927417755127\n",">>> time: [09:37:28] training ep: 0 - batch(i): 340 , loss: 0.3814835548400879\n",">>> time: [09:37:29] training ep: 0 - batch(i): 350 , loss: 0.4548397362232208\n",">>> time: [09:37:30] training ep: 0 - batch(i): 360 , loss: 0.4524921476840973\n",">>> time: [09:37:31] training ep: 0 - batch(i): 370 , loss: 0.27913546562194824\n",">>> time: [09:37:32] training ep: 0 - batch(i): 380 , loss: 0.498452752828598\n",">>> time: [09:37:33] training ep: 0 - batch(i): 390 , loss: 0.41255825757980347\n",">>> time: [09:37:34] training ep: 0 - batch(i): 400 , loss: 0.24730344116687775\n",">>> time: [09:37:35] training ep: 0 - batch(i): 410 , loss: 0.4716986417770386\n",">>> time: [09:37:36] training ep: 0 - batch(i): 420 , loss: 0.2180049866437912\n",">>> time: [09:37:37] training ep: 0 - batch(i): 430 , loss: 0.5068372488021851\n",">>> time: [09:37:38] training ep: 0 - batch(i): 440 , loss: 0.5999781489372253\n",">>> time: [09:37:38] training ep: 0 - batch(i): 450 , loss: 0.6020120978355408\n",">>> time: [09:37:39] training ep: 0 - batch(i): 460 , loss: 0.35303395986557007\n",">>> time: [09:37:40] training ep: 0 - batch(i): 470 , loss: 0.2110758125782013\n",">>> time: [09:37:41] training ep: 0 - batch(i): 480 , loss: 0.5578760504722595\n",">>> time: [09:37:42] training ep: 0 - batch(i): 490 , loss: 0.39329585433006287\n",">>> time: [09:37:43] training ep: 0 - batch(i): 500 , loss: 0.2690660059452057\n",">>> time: [09:37:44] training ep: 0 - batch(i): 510 , loss: 0.5511594414710999\n",">>> time: [09:37:45] training ep: 0 - batch(i): 520 , loss: 0.5059325098991394\n",">>> time: [09:37:46] training ep: 0 - batch(i): 530 , loss: 0.24301926791667938\n",">>> time: [09:37:47] training ep: 0 - batch(i): 540 , loss: 0.39328667521476746\n",">>> time: [09:37:47] training ep: 0 - batch(i): 550 , loss: 0.6085259318351746\n",">>> time: [09:37:48] training ep: 0 - batch(i): 560 , loss: 0.46837329864501953\n",">>> time: [09:37:49] training ep: 0 - batch(i): 570 , loss: 0.232101708650589\n",">>> time: [09:37:50] training ep: 0 - batch(i): 580 , loss: 0.5708605051040649\n",">>> time: [09:37:51] training ep: 0 - batch(i): 590 , loss: 0.48368197679519653\n",">>> time: [09:37:52] training ep: 0 - batch(i): 600 , loss: 0.7030518651008606\n",">>> time: [09:37:53] training ep: 0 - batch(i): 610 , loss: 0.7257893681526184\n",">>> time: [09:37:54] training ep: 0 - batch(i): 620 , loss: 0.3076539933681488\n",">>> time: [09:37:55] training ep: 0 - batch(i): 630 , loss: 0.5132489204406738\n",">>> time: [09:37:56] training ep: 0 - batch(i): 640 , loss: 0.359712690114975\n",">>> time: [09:37:56] training ep: 0 - batch(i): 650 , loss: 0.2663463056087494\n",">>> time: [09:37:57] training ep: 0 - batch(i): 660 , loss: 0.1287960559129715\n",">>> time: [09:37:58] training ep: 0 - batch(i): 670 , loss: 0.6363756656646729\n",">>> time: [09:37:59] training ep: 0 - batch(i): 680 , loss: 0.4613933265209198\n",">>> time: [09:38:00] training ep: 0 - batch(i): 690 , loss: 0.39450007677078247\n",">>> time: [09:38:01] training ep: 0 - batch(i): 700 , loss: 0.11280084401369095\n",">>> time: [09:38:02] training ep: 0 - batch(i): 710 , loss: 0.4315088391304016\n",">>> time: [09:38:03] training ep: 0 - batch(i): 720 , loss: 0.3881022334098816\n",">>> time: [09:38:04] training ep: 0 - batch(i): 730 , loss: 0.5425661206245422\n",">>> time: [09:38:05] training ep: 0 - batch(i): 740 , loss: 0.3288578689098358\n",">>> time: [09:38:06] training ep: 0 - batch(i): 750 , loss: 0.34980204701423645\n",">>> time: [09:38:06] training ep: 0 - batch(i): 760 , loss: 0.3800355792045593\n",">>> time: [09:38:07] training ep: 0 - batch(i): 770 , loss: 0.5688133239746094\n",">>> time: [09:38:08] training ep: 0 - batch(i): 780 , loss: 0.5477626919746399\n",">>> time: [09:38:09] training ep: 0 - batch(i): 790 , loss: 0.4300270080566406\n",">>> time: [09:38:10] training ep: 0 - batch(i): 800 , loss: 0.3899056017398834\n",">>> time: [09:38:11] training ep: 0 - batch(i): 810 , loss: 0.8412011861801147\n",">>> time: [09:38:12] training ep: 0 - batch(i): 820 , loss: 0.39316749572753906\n",">>> time: [09:38:13] training ep: 0 - batch(i): 830 , loss: 0.22031724452972412\n",">>> time: [09:38:14] training ep: 0 - batch(i): 840 , loss: 0.2950463891029358\n",">>> time: [09:38:15] training ep: 0 - batch(i): 850 , loss: 0.24895592033863068\n",">>> time: [09:38:15] training ep: 0 - batch(i): 860 , loss: 0.39163926243782043\n",">>> time: [09:38:16] training ep: 0 - batch(i): 870 , loss: 0.24615995585918427\n",">>> time: [09:38:17] training ep: 0 - batch(i): 880 , loss: 0.22283151745796204\n",">>> time: [09:38:18] training ep: 0 - batch(i): 890 , loss: 0.4047279953956604\n",">>> time: [09:38:19] training ep: 0 - batch(i): 900 , loss: 0.23784179985523224\n",">>> time: [09:38:20] training ep: 0 - batch(i): 910 , loss: 0.37876734137535095\n",">>> time: [09:38:21] training ep: 0 - batch(i): 920 , loss: 0.8523074984550476\n",">>> time: [09:38:22] training ep: 0 - batch(i): 930 , loss: 0.20803947746753693\n",">>> time: [09:38:23] training ep: 0 - batch(i): 940 , loss: 0.4762040376663208\n",">>> time: [09:38:23] training ep: 0 - batch(i): 950 , loss: 0.31995177268981934\n",">>> time: [09:38:24] training ep: 0 - batch(i): 960 , loss: 0.4138440787792206\n",">>> time: [09:38:25] training ep: 0 - batch(i): 970 , loss: 0.8184276819229126\n",">>> time: [09:38:26] training ep: 0 - batch(i): 980 , loss: 0.15990526974201202\n",">>> time: [09:38:27] training ep: 0 - batch(i): 990 , loss: 0.20343318581581116\n",">>> time: [09:38:28] training ep: 0 - batch(i): 1000 , loss: 0.4051169753074646\n",">>> time: [09:38:29] training ep: 0 - batch(i): 1010 , loss: 0.29794320464134216\n",">>> time: [09:38:30] training ep: 0 - batch(i): 1020 , loss: 0.21249689161777496\n",">>> time: [09:38:31] training ep: 0 - batch(i): 1030 , loss: 0.3634692132472992\n",">>> time: [09:38:32] training ep: 0 - batch(i): 1040 , loss: 0.3528178036212921\n",">>> time: [09:38:33] training ep: 0 - batch(i): 1050 , loss: 0.2931833565235138\n",">>> time: [09:38:33] training ep: 0 - batch(i): 1060 , loss: 0.5241166353225708\n",">>> time: [09:38:34] training ep: 0 - batch(i): 1070 , loss: 0.36639708280563354\n",">>> time: [09:38:35] training ep: 0 - batch(i): 1080 , loss: 0.22763507068157196\n",">>> time: [09:38:36] training ep: 0 - batch(i): 1090 , loss: 0.3421734571456909\n",">>> time: [09:38:37] training ep: 0 - batch(i): 1100 , loss: 0.460666686296463\n",">>> time: [09:38:38] training ep: 0 - batch(i): 1110 , loss: 0.3330051898956299\n",">>> time: [09:38:39] training ep: 0 - batch(i): 1120 , loss: 0.456409215927124\n",">>> time: [09:38:40] training ep: 0 - batch(i): 1130 , loss: 0.3886191248893738\n",">>> time: [09:38:41] training ep: 0 - batch(i): 1140 , loss: 0.39431294798851013\n",">>> time: [09:38:42] training ep: 0 - batch(i): 1150 , loss: 0.43698129057884216\n",">>> time: [09:38:42] training ep: 0 - batch(i): 1160 , loss: 0.2693729102611542\n",">>> time: [09:38:43] training ep: 0 - batch(i): 1170 , loss: 0.38228297233581543\n",">>> time: [09:38:44] training ep: 0 - batch(i): 1180 , loss: 0.38350731134414673\n",">>> time: [09:38:45] training ep: 0 - batch(i): 1190 , loss: 0.35603559017181396\n",">>> time: [09:38:46] training ep: 0 - batch(i): 1200 , loss: 1.9539114236831665\n",">>> time: [09:38:47] training ep: 0 - batch(i): 1210 , loss: 0.4916232228279114\n",">>> time: [09:38:48] training ep: 0 - batch(i): 1220 , loss: 0.22715570032596588\n",">>> time: [09:38:49] training ep: 0 - batch(i): 1230 , loss: 0.3236835300922394\n",">>> time: [09:38:50] training ep: 0 - batch(i): 1240 , loss: 0.21227586269378662\n",">>> time: [09:38:51] training ep: 0 - batch(i): 1250 , loss: 0.4999234676361084\n",">>> time: [09:38:51] training ep: 0 - batch(i): 1260 , loss: 0.3290454149246216\n",">>> time: [09:38:52] training ep: 0 - batch(i): 1270 , loss: 0.16331364214420319\n",">>> time: [09:38:53] training ep: 0 - batch(i): 1280 , loss: 0.36781200766563416\n",">>> time: [09:38:54] training ep: 0 - batch(i): 1290 , loss: 1.1863855123519897\n",">>> time: [09:38:55] training ep: 0 - batch(i): 1300 , loss: 0.23375177383422852\n",">>> time: [09:38:56] training ep: 0 - batch(i): 1310 , loss: 0.6765826940536499\n",">>> time: [09:38:57] training ep: 0 - batch(i): 1320 , loss: 0.6510344743728638\n",">>> time: [09:38:58] training ep: 0 - batch(i): 1330 , loss: 0.11588514596223831\n",">>> time: [09:38:59] training ep: 0 - batch(i): 1340 , loss: 0.45600712299346924\n",">>> time: [09:39:00] training ep: 0 - batch(i): 1350 , loss: 0.35864293575286865\n",">>> time: [09:39:00] training ep: 0 - batch(i): 1360 , loss: 0.2079397290945053\n",">>> time: [09:39:01] training ep: 0 - batch(i): 1370 , loss: 0.2544628381729126\n",">>> time: [09:39:02] training ep: 0 - batch(i): 1380 , loss: 0.5540197491645813\n",">>> time: [09:39:03] training ep: 0 - batch(i): 1390 , loss: 0.21192477643489838\n",">>> time: [09:39:04] training ep: 0 - batch(i): 1400 , loss: 0.371130108833313\n",">>> time: [09:39:05] training ep: 0 - batch(i): 1410 , loss: 0.24084331095218658\n",">>> time: [09:39:06] training ep: 0 - batch(i): 1420 , loss: 0.25300297141075134\n",">>> time: [09:39:07] training ep: 0 - batch(i): 1430 , loss: 0.3260328769683838\n",">>> time: [09:39:08] training ep: 0 - batch(i): 1440 , loss: 0.3316088020801544\n",">>> time: [09:39:09] training ep: 0 - batch(i): 1450 , loss: 0.3408605754375458\n",">>> time: [09:39:09] training ep: 0 - batch(i): 1460 , loss: 0.27737709879875183\n",">>> time: [09:39:10] training ep: 0 - batch(i): 1470 , loss: 0.39953404664993286\n",">>> time: [09:39:11] training ep: 0 - batch(i): 1480 , loss: 0.1593734472990036\n",">>> time: [09:39:12] training ep: 0 - batch(i): 1490 , loss: 0.25535276532173157\n",">>> time: [09:39:13] training ep: 0 - batch(i): 1500 , loss: 0.25332820415496826\n",">>> time: [09:39:14] training ep: 0 - batch(i): 1510 , loss: 0.22068682312965393\n",">>> time: [09:39:15] training ep: 0 - batch(i): 1520 , loss: 0.2499922215938568\n",">>> time: [09:39:16] training ep: 0 - batch(i): 1530 , loss: 0.31827008724212646\n",">>> time: [09:39:17] training ep: 0 - batch(i): 1540 , loss: 0.27378493547439575\n",">>> time: [09:39:18] training ep: 0 - batch(i): 1550 , loss: 0.2944931089878082\n",">>> time: [09:39:18] training ep: 0 - batch(i): 1560 , loss: 0.40390118956565857\n",">>> time: [09:39:19] training ep: 0 - batch(i): 1570 , loss: 0.49934911727905273\n",">>> time: [09:39:20] training ep: 0 - batch(i): 1580 , loss: 0.113308846950531\n",">>> time: [09:39:21] training ep: 0 - batch(i): 1590 , loss: 0.13120737671852112\n",">>> time: [09:39:22] training ep: 0 - batch(i): 1600 , loss: 0.3386583626270294\n",">>> time: [09:39:23] training ep: 0 - batch(i): 1610 , loss: 0.309072881937027\n",">>> time: [09:39:24] training ep: 0 - batch(i): 1620 , loss: 0.3271205723285675\n",">>> time: [09:39:25] training ep: 0 - batch(i): 1630 , loss: 0.29630717635154724\n",">>> time: [09:39:26] training ep: 0 - batch(i): 1640 , loss: 0.7975636720657349\n",">>> time: [09:39:27] training ep: 0 - batch(i): 1650 , loss: 0.283109188079834\n",">>> time: [09:39:28] training ep: 0 - batch(i): 1660 , loss: 0.18827831745147705\n",">>> time: [09:39:28] training ep: 0 - batch(i): 1670 , loss: 0.2265186607837677\n",">>> time: [09:39:29] training ep: 0 - batch(i): 1680 , loss: 0.39828360080718994\n",">>> time: [09:39:30] training ep: 0 - batch(i): 1690 , loss: 0.2253480702638626\n",">>> time: [09:39:31] training ep: 0 - batch(i): 1700 , loss: 0.20549483597278595\n",">>> time: [09:39:32] training ep: 0 - batch(i): 1710 , loss: 0.3287575840950012\n",">>> time: [09:39:33] training ep: 0 - batch(i): 1720 , loss: 0.8202905058860779\n",">>> time: [09:39:34] training ep: 0 - batch(i): 1730 , loss: 0.2480040192604065\n",">>> time: [09:39:35] training ep: 0 - batch(i): 1740 , loss: 0.13948430120944977\n",">>> time: [09:39:36] training ep: 0 - batch(i): 1750 , loss: 0.23497411608695984\n",">>> time: [09:39:37] training ep: 0 - batch(i): 1760 , loss: 0.21581538021564484\n",">>> time: [09:39:38] training ep: 0 - batch(i): 1770 , loss: 0.23627175390720367\n",">>> time: [09:39:38] training ep: 0 - batch(i): 1780 , loss: 0.25786063075065613\n",">>> time: [09:39:39] training ep: 0 - batch(i): 1790 , loss: 0.32140856981277466\n",">>> time: [09:39:40] training ep: 0 - batch(i): 1800 , loss: 0.16353145241737366\n",">>> time: [09:39:41] training ep: 0 - batch(i): 1810 , loss: 0.2141086757183075\n",">>> time: [09:39:42] training ep: 0 - batch(i): 1820 , loss: 0.23832280933856964\n",">>> time: [09:39:43] training ep: 0 - batch(i): 1830 , loss: 0.25199243426322937\n",">>> time: [09:39:44] training ep: 0 - batch(i): 1840 , loss: 0.34495478868484497\n",">>> time: [09:39:45] training ep: 0 - batch(i): 1850 , loss: 0.14809125661849976\n",">>> time: [09:39:46] training ep: 0 - batch(i): 1860 , loss: 0.327534556388855\n",">>> time: [09:39:47] training ep: 0 - batch(i): 1870 , loss: 0.2427365630865097\n",">>> time: [09:39:48] training ep: 0 - batch(i): 1880 , loss: 0.2233964055776596\n",">>> time: [09:39:48] training ep: 0 - batch(i): 1890 , loss: 0.28790923953056335\n",">>> time: [09:39:49] training ep: 0 - batch(i): 1900 , loss: 0.1177501305937767\n",">>> time: [09:39:50] training ep: 0 - batch(i): 1910 , loss: 0.26679515838623047\n",">>> time: [09:39:51] training ep: 0 - batch(i): 1920 , loss: 0.6180666089057922\n",">>> time: [09:39:52] training ep: 0 - batch(i): 1930 , loss: 0.28195422887802124\n",">>> time: [09:39:53] training ep: 0 - batch(i): 1940 , loss: 0.18995535373687744\n",">>> time: [09:39:54] training ep: 0 - batch(i): 1950 , loss: 0.34841635823249817\n",">>> time: [09:39:55] training ep: 0 - batch(i): 1960 , loss: 0.31635111570358276\n",">>> time: [09:39:56] training ep: 0 - batch(i): 1970 , loss: 0.12970100343227386\n",">>> time: [09:39:57] training ep: 0 - batch(i): 1980 , loss: 0.18783637881278992\n",">>> time: [09:39:58] training ep: 0 - batch(i): 1990 , loss: 0.15988172590732574\n",">>> time: [09:39:58] training ep: 0 - batch(i): 2000 , loss: 0.2362634539604187\n",">>> time: [09:39:59] training ep: 0 - batch(i): 2010 , loss: 0.17448443174362183\n",">>> time: [09:40:00] training ep: 0 - batch(i): 2020 , loss: 0.3823540210723877\n",">>> time: [09:40:01] training ep: 0 - batch(i): 2030 , loss: 0.1431361436843872\n",">>> time: [09:40:02] training ep: 0 - batch(i): 2040 , loss: 0.25888490676879883\n",">>> time: [09:40:03] training ep: 0 - batch(i): 2050 , loss: 0.12757740914821625\n",">>> time: [09:40:04] training ep: 0 - batch(i): 2060 , loss: 0.21519719064235687\n",">>> time: [09:40:05] training ep: 0 - batch(i): 2070 , loss: 0.37466883659362793\n",">>> time: [09:40:06] training ep: 0 - batch(i): 2080 , loss: 0.10377120971679688\n",">>> time: [09:40:07] training ep: 0 - batch(i): 2090 , loss: 0.5273855924606323\n",">>> time: [09:40:08] training ep: 0 - batch(i): 2100 , loss: 0.24509359896183014\n",">>> time: [09:40:08] training ep: 0 - batch(i): 2110 , loss: 0.14265304803848267\n",">>> time: [09:40:09] training ep: 0 - batch(i): 2120 , loss: 0.21979773044586182\n",">>> time: [09:40:10] training ep: 0 - batch(i): 2130 , loss: 0.25515592098236084\n",">>> time: [09:40:11] training ep: 0 - batch(i): 2140 , loss: 0.3176818788051605\n",">>> time: [09:40:12] training ep: 0 - batch(i): 2150 , loss: 0.2354404330253601\n",">>> time: [09:40:13] training ep: 0 - batch(i): 2160 , loss: 0.18827281892299652\n",">>> time: [09:40:14] training ep: 0 - batch(i): 2170 , loss: 0.24875672161579132\n",">>> time: [09:40:15] training ep: 0 - batch(i): 2180 , loss: 0.16831856966018677\n",">>> time: [09:40:16] training ep: 0 - batch(i): 2190 , loss: 0.15826183557510376\n",">>> time: [09:40:17] training ep: 0 - batch(i): 2200 , loss: 0.18563641607761383\n",">>> time: [09:40:18] training ep: 0 - batch(i): 2210 , loss: 0.2377406507730484\n",">>> time: [09:40:18] training ep: 0 - batch(i): 2220 , loss: 0.10399027168750763\n",">>> time: [09:40:19] training ep: 0 - batch(i): 2230 , loss: 0.14929847419261932\n",">>> time: [09:40:20] training ep: 0 - batch(i): 2240 , loss: 0.26681333780288696\n","training complete, save results? [y/n] : y\n","command not recognised, enter y or n : y\n","saving weights to weights/...\n","weights and field pickles saved to weights\n","train for more epochs? [y/n] : n\n","exiting program...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"d9lsaJxN0DKa"},"source":["!python3 -m spacy download en && spacy download vi\n","!python3 -m pip install pyvi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4mUGyTaj5XSv"},"source":["### Chạy đoạn code bên dưới.\n","Lưu ý: Nếu huấn luyện dữ liệu với mô hình có sẵn thì thêm \"-load_weights weights\""]},{"cell_type":"code","metadata":{"id":"sGplfY55z55S"},"source":["!python3 train.py -SGDR 1 -floyd -batchsize 512 -checkpoint 20 -epochs 5 -printevery 10 -max_strlen 100 -src_data 'data/vi-0203-no-augment-shuffle.txt' -trg_data 'data/bana-0203-no-augment-shuffle.txt' -src_lang vi -trg_lang en"],"execution_count":null,"outputs":[]}]}